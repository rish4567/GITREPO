{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e2fd8e",
   "metadata": {},
   "source": [
    "# Fraud Detection with Logistic Regression and Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c6d501",
   "metadata": {},
   "source": [
    "# You are a data scientist at a financial institution, and your primary task is to develop a fraud detection model using logistic regression. The dataset you have is highly Imbalanced, with only a small fraction of transactions being fraudulent. Your objective is to create an effective model by implementing logistic regression and employing various feature engineering techniques to improve the model's performance:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6445c6",
   "metadata": {},
   "source": [
    "# 1. Data Preparation:\n",
    "\n",
    "a. Load the dataset, and provide an overview of the available features, including transaction details, customer information, and labels (fraudulent or non-fraudulent).\n",
    "\n",
    "b. Describe the class distribution of fraudulent and non-fraudulent transactions and discuss the imbalance issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a92195",
   "metadata": {},
   "source": [
    "a. To begin developing a fraud detection model using logistic regression, you'll first need to load the dataset and provide an overview of the available features, including transaction details, customer information, and labels (fraudulent or non-fraudulent). Here's a general outline of the steps to take:\n",
    "\n",
    "i. Load the Dataset: Import the dataset into your data science environment, whether it's Python, R, or another language of your choice. You can use libraries like pandas to read and manipulate the data.\n",
    "\n",
    "ii. Explore the Features: Inspect the dataset to understand the features available. Features typically include transaction-related information (e.g., transaction amount, timestamp, merchant ID) and customer-related information (e.g., customer ID, demographics). You should also have a binary label column indicating whether a transaction is fraudulent (1) or non-fraudulent (0).\n",
    "\n",
    "iii. Data Overview: Calculate summary statistics for the features, such as mean, median, and standard deviation, to understand the data distribution. Additionally, check for missing values and data types of each feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357e2287",
   "metadata": {},
   "source": [
    "b. The class distribution of fraudulent and non-fraudulent transactions is crucial to address because imbalanced datasets can lead to model bias. Describe the class distribution and discuss the imbalance issue:\n",
    "\n",
    "i. Class Distribution: Calculate the number of fraudulent (label=1) and non-fraudulent (label=0) transactions. You can use a simple count or a histogram to visualize the distribution.\n",
    "\n",
    "ii. Imbalance Issue: Imbalanced datasets can pose challenges for machine learning models because they tend to bias the model towards the majority class (non-fraudulent transactions). In fraud detection, the majority of transactions are typically non-fraudulent, making the dataset highly imbalanced. This can lead to a model that performs poorly in identifying fraudulent transactions.\n",
    "\n",
    "iii. Consequences of Imbalance: Discuss the consequences of an imbalanced dataset, such as lower accuracy in identifying fraud, difficulty in learning patterns for the minority class, and the potential for high false negatives.\n",
    "\n",
    "iv. Strategies to Address Imbalance: Explain that various strategies can be used to address the class imbalance issue, including oversampling the minority class (fraudulent transactions), undersampling the majority class (non-fraudulent transactions), and using techniques like Synthetic Minority Over-sampling Technique (SMOTE). These strategies can help create a balanced dataset for model training.\n",
    "\n",
    "v. Performance Metric Choice: Consider using performance metrics such as precision, recall, F1-score, and area under the ROC curve (AUC-ROC) that are more suitable for imbalanced datasets compared to accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fe675f",
   "metadata": {},
   "source": [
    "# 2. Initial Logistic Regression Model:\n",
    "\n",
    "a. Implement a basic logistic regression model using the raw dataset.\n",
    "\n",
    "b. Evaluate the model's performance using standard metrics like accuracy, precision, recall, and F1-score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "data = pd.read_csv(\"your_dataset.csv\")\n",
    "X = data.drop(\"label\", axis=1) \n",
    "y = data[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eec29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9985c73",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering:\n",
    "\n",
    "a. Apply feature engineering techniques to enhance the predictive power of the model. These techniques may include:\n",
    "\n",
    "-Creating new features.\n",
    "\n",
    "-Scaling or normalizing features.\n",
    "\n",
    "-Handling missing values.\n",
    "\n",
    "-Encoding categorical variables.\n",
    "\n",
    "b. Explain why each feature engineering technique is relevant for fraudÂ detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c1e040",
   "metadata": {},
   "source": [
    "1.Creating New Features:\n",
    "\n",
    "Relevance: Creating new features, often referred to as feature generation, is highly relevant in fraud detection. New features can capture complex relationships between existing variables or provide domain-specific insights. For example, you can create features like the time since the last transaction, the frequency of high-value transactions, or the velocity of transactions (e.g., the number of transactions within a certain time window). These new features can help the model uncover patterns that may not be evident in the raw data.\n",
    "\n",
    "2.Scaling or Normalizing Features:\n",
    "\n",
    "Relevance: Scaling or normalizing features is essential for fraud detection, especially when dealing with numerical attributes like transaction amounts. Normalizing these features ensures that they have the same scale, making it easier for the model to learn and generalize from the data. Logistic regression, for instance, relies on linear combinations of features, and having features on a similar scale helps the model converge more efficiently.\n",
    "\n",
    "3.Handling Missing Values:\n",
    "\n",
    "Relevance: Missing values can be problematic for any machine learning model, including logistic regression. In fraud detection, missing values may indicate issues with data quality or inform the model about potential anomalies. The appropriate handling of missing values, whether by imputation or treating them as a separate category, can prevent the model from making incorrect assumptions and enhance its ability to identify fraudulent transactions.\n",
    "\n",
    "4.Encoding Categorical Variables:\n",
    "\n",
    "Relevance: Categorical variables like merchant IDs, customer IDs, or transaction types are common in fraud detection datasets. These need to be converted into numerical representations for the model to use them effectively. Various encoding techniques, such as one-hot encoding or label encoding, are relevant because they transform categorical variables into a format that logistic regression can work with. This step ensures that valuable categorical information is not lost and can be used for fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54f0188",
   "metadata": {},
   "source": [
    "# 4. Handling Imbalanced Data\n",
    "\n",
    "a. Discuss the challenges associated with imbalanced datasets in the context of fraud detection,\n",
    "\n",
    "b. Implement strategies to address class imbalance, such as:\n",
    "\n",
    "-Oversampling the minority class.\n",
    "\n",
    "-Undersampling the majority class\n",
    "\n",
    "-Using synthetic data generation techniques (eg, SMOTE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055c1f6d",
   "metadata": {},
   "source": [
    "#Answer\n",
    "a. Challenges Associated with Imbalanced Datasets in Fraud Detection:\n",
    "\n",
    "In the context of fraud detection, imbalanced datasets pose several challenges:\n",
    "\n",
    "i. Bias towards the Majority Class: Machine learning models tend to be biased towards the majority class (non-fraudulent transactions) because they have more data points to learn from. As a result, the model may have a high accuracy but perform poorly in identifying the minority class (fraudulent transactions).\n",
    "\n",
    "ii. High False Negative Rate: Detecting fraudulent transactions is often the primary goal. An imbalanced dataset may lead to a high false negative rate, where the model fails to identify actual fraud cases, which can be costly and damaging.\n",
    "\n",
    "iii. Model Generalization Issues: Imbalanced datasets can lead to models that overfit to the majority class, making them less capable of generalizing to new, unseen data.\n",
    "\n",
    "iv. Evaluation Bias: Common evaluation metrics like accuracy can be misleading in imbalanced datasets. The model may achieve a high accuracy by predicting most transactions as non-fraudulent, even if it misses many fraudulent ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f8c625",
   "metadata": {},
   "source": [
    "b. Strategies to Address Class Imbalance:\n",
    "\n",
    "i. Oversampling the Minority Class:\n",
    "- Relevance: Oversampling involves increasing the number of instances in the minority class (fraudulent transactions) by either replicating existing samples or generating synthetic ones. This helps the model see more examples of the minority class, making it better at identifying fraud.\n",
    "- Implementation: Libraries like imbalanced-learn in Python provide various oversampling techniques, such as Random Oversampling, SMOTE (Synthetic Minority Over-sampling Technique), and ADASYN (Adaptive Synthetic Sampling). SMOTE is widely used and effective. It generates synthetic instances by interpolating between existing minority class samples.\n",
    "\n",
    "ii. Undersampling the Majority Class:\n",
    "- Relevance: Undersampling aims to reduce the number of instances in the majority class. By reducing the number of non-fraudulent transactions, the model may become more balanced and less biased towards the majority class.\n",
    "- Implementation: Undersampling can be performed by randomly selecting a subset of non-fraudulent transactions or using more advanced techniques like Tomek links or edited nearest neighbors. However, undersampling may lead to loss of information, so it should be used cautiously.\n",
    "\n",
    "iii. Using Synthetic Data Generation Techniques (e.g., SMOTE):\n",
    "- Relevance: Synthetic data generation techniques, like SMOTE, create new instances for the minority class by interpolating between existing samples. This approach increases the effective size of the minority class without replicating data, addressing the imbalance issue.\n",
    "- Implementation: SMOTE can be implemented using libraries like imbalanced-learn in Python. It selects a random minority class instance, identifies its k-nearest neighbors, and generates synthetic instances along the line connecting the original instance and its neighbors. SMOTE can be a powerful tool to balance the dataset and improve model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b8fe7f",
   "metadata": {},
   "source": [
    "# 5. Logistic Regression with Feature-Engineered Data:\n",
    "\n",
    "a. Train a logistic regression model using the feature-engineered dataset and the methods for handling imbalanced data.\n",
    "\n",
    "b. Evaluate the model's performance using appropriate evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f539d751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "data = pd.read_csv(\"feature_engineered_dataset.csv\")\n",
    "X = data.drop(\"label\", axis=1) \n",
    "y = data[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "undersampler = RandomUnderSampler(sampling_strategy=0.5, random_state=42)  \n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train_resampled, y_train_resampled)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75937610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb8e56",
   "metadata": {},
   "source": [
    "# 6. Model Interpretation:\n",
    "\n",
    "a. Interpret the coefficients of the logistic regression model and discuss which features have the most influence on fraud detection.\n",
    "\n",
    "b. Explain how the logistic regression model can be used for decision-making in identifying potential fraud.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bba4ba",
   "metadata": {},
   "source": [
    "a. Interpreting the Coefficients of the Logistic Regression Model:\n",
    "\n",
    "Logistic regression provides interpretable coefficients that can help identify which features have the most influence on fraud detection. The coefficients represent the change in the log-odds of the target variable for a one-unit change in the corresponding predictor variable. Here's how you can interpret the coefficients:\n",
    "\n",
    "Positive Coefficients: Features with positive coefficients increase the log-odds of the target variable. This means that an increase in the feature's value makes it more likely that a transaction is fraudulent.\n",
    "\n",
    "Negative Coefficients: Features with negative coefficients decrease the log-odds of the target variable. A decrease in the feature's value makes it less likely that a transaction is fraudulent.\n",
    "\n",
    "Magnitude of Coefficients: The magnitude of the coefficients indicates the strength of the influence. Larger absolute values of coefficients suggest a stronger impact on the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e9e35b",
   "metadata": {},
   "source": [
    "b. Using the Logistic Regression Model for Decision-Making in Identifying Potential Fraud:\n",
    "\n",
    "Logistic regression can be used for decision-making in identifying potential fraud in the following ways:\n",
    "\n",
    "Probability Threshold: By choosing an appropriate probability threshold, you can make decisions about classifying transactions as potential fraud or non-fraud. For example, if the model predicts a probability greater than 0.5, you can classify it as potential fraud. The choice of threshold depends on your risk tolerance and business requirements.\n",
    "\n",
    "Scoring and Ranking: You can use the predicted probabilities to score and rank transactions based on the likelihood of being fraudulent. High-probability transactions can be flagged for further investigation.\n",
    "\n",
    "Alert Generation: The logistic regression model can be integrated into an alert system. When a transaction's predicted probability exceeds a certain threshold, an alert can be triggered for manual review by fraud analysts.\n",
    "\n",
    "Fraud Prevention: Logistic regression can also be used for real-time fraud prevention. If a transaction is deemed highly likely to be fraudulent, it can be automatically blocked or subjected to additional verification steps to prevent unauthorized transactions.\n",
    "\n",
    "Model Feedback and Updates: Periodically, the model can be retrained with new data to adapt to changing fraud patterns and improve its performance. Feedback from manual reviews and the outcome of flagged transactions can be used to update the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aaf666",
   "metadata": {},
   "source": [
    "# 7. Model Comparison:\n",
    "\n",
    "a. Compare the performance of the initial logistic regression model with the feature-engineered and balanced data model.\n",
    "\n",
    "b. Discuss the advantages and limitations of each approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bd42b7",
   "metadata": {},
   "source": [
    "a. Comparing the Performance of Models:\n",
    "    Initial Logistic Regression Model (Unbalanced Data):\n",
    "\n",
    "Advantages:\n",
    "Simplicity: Easy to implement and interpret.\n",
    "Quick to train on the dataset.\n",
    "\n",
    "Limitations:\n",
    "Imbalanced data affects performance.\n",
    "High false negative rate in fraud detection.\n",
    "May not effectively capture fraud patterns due to data imbalance.\n",
    "\n",
    "Logistic Regression Model with Feature-Engineered and Balanced Data:\n",
    "\n",
    "Advantages:\n",
    "Improved performance: Better recall and ability to identify fraudulent transactions.\n",
    "Utilizes feature engineering to capture more patterns.\n",
    "Addresses class imbalance using techniques like SMOTE or undersampling.\n",
    "\n",
    "Limitations:\n",
    "May require more preprocessing and computation.\n",
    "Balancing the data can lead to more false positives, affecting precision.\n",
    "Interpretability can be more challenging with feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ba5e8a",
   "metadata": {},
   "source": [
    "b. Advantages and Limitations of Each Approach:\n",
    "\n",
    "Advantages of the Initial Logistic Regression Model (Unbalanced Data):\n",
    "\n",
    "Simplicity: The model is easy to understand and implement.\n",
    "Quick Training: It's faster to train on the dataset due to its simplicity.\n",
    "Limitations of the Initial Logistic Regression Model (Unbalanced Data):\n",
    "\n",
    "Imbalanced Data: Imbalanced datasets lead to a biased model with high false negatives.\n",
    "Missed Fraud: The model is likely to miss many fraudulent transactions.\n",
    "Advantages of the Logistic Regression Model with Feature-Engineered and Balanced Data:\n",
    "\n",
    "Improved Performance: It addresses the class imbalance issue and offers better recall, reducing the risk of missing fraud.\n",
    "Utilizes Feature Engineering: Feature engineering captures more fraud patterns.\n",
    "Flexibility: You can fine-tune the model to balance false positives and false negatives according to business requirements.\n",
    "Limitations of the Logistic Regression Model with Feature-Engineered and Balanced Data:\n",
    "\n",
    "Preprocessing: It may require more data preprocessing and computational resources.\n",
    "Increased False Positives: Balancing the data can lead to more false positives, affecting precision.\n",
    "Interpretability: The model's interpretability may be somewhat reduced due to the complexity introduced by feature engineering.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
