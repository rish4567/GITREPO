{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adbe3ac3",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "\n",
    "A new car manufacturing company wants to launch cars in different categories and would like to have an idea about the price of cars based on cars specifications (in terms of various characteristics) available in market.\n",
    "\n",
    "Task: Develop a suitable model which can help company to predict car prices.\n",
    "\n",
    "Following points must be included in your analysis:\n",
    "\n",
    "⚫EDA analysis (Exploratory Data Analysis):\n",
    "\n",
    "> Need to present complete data review with suitable charts/graphs.\n",
    "\n",
    "• Data processing steps\n",
    "\n",
    "Generic steps and process followed for given dataset)-\n",
    "\n",
    "For example, if one of the missing value treatments has been applied then we would need information on other methods as well with justification as to why this method has been applied instead of others. This would be applicable for all steps (like multicollinearity, outlier, variable selection etc.) you followed in data processing/preparation.\n",
    "\n",
    "Need justification if any Variable transformation (Bucketing, dummy variable creation) has been applied.\n",
    "\n",
    "Assumption applied, if any.\n",
    "\n",
    "• Model Building:\n",
    "\n",
    "Reason for selecting this model (what are the criteria you considered to finalize your model, also provide generic ranges of considered criteria.\n",
    "\n",
    "Considered model selection criteria, also give information on criteria you have not considered but can be considered.\n",
    "\n",
    "Results of the model using Test and Validation sample.\n",
    "\n",
    "Submission Details:\n",
    "\n",
    "1. You are supposed to share python code along with above mentioned details.\n",
    "\n",
    "2. You are supposed to share test dataset along with predicted prices based upon the model built using training dataset.\n",
    "\n",
    "3. Perform the Lazzo & Ridge Optimization model.\n",
    "\n",
    "4. Build the various ensemble model and observe the performance.\n",
    "\n",
    "5. Prepare a presentation or report summarizing your analysis, results, and recommendations for the retail company."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac88a607",
   "metadata": {},
   "source": [
    "#Solutions\n",
    "\n",
    "\n",
    "To address the problem statement and complete the tasks you've mentioned, you'll need to follow a structured process that includes data analysis, data preprocessing, model building, model evaluation, and reporting. Here's a step-by-step guide to help you complete this project:\n",
    "\n",
    "Step 1: Data Collection\n",
    "Assuming you already have the dataset, load the data into your Python environment. The dataset should contain information about car specifications and their corresponding prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64452b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"cars_test.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1f079",
   "metadata": {},
   "source": [
    "Step 2: Exploratory Data Analysis (EDA)\n",
    "Perform a thorough EDA to understand the dataset. This includes:\n",
    "\n",
    "Summarizing statistics (mean, median, standard deviation, etc.) for numerical features.\n",
    "Visualizing data using various plots and graphs to understand relationships and distributions.\n",
    "Handling missing values and justifying the method used (e.g., imputation, removal, etc.).\n",
    "Handling outliers and justifying the method used (e.g., clipping, transformation, etc.).\n",
    "Identifying and addressing multicollinearity if present.\n",
    "Feature selection or engineering if needed.\n",
    "Example EDA steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd202f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "data.dropna(inplace=True)\n",
    "correlation_matrix = data.corr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba0386d",
   "metadata": {},
   "source": [
    "Step 3: Data Preprocessing\n",
    "Prepare the data for model building by performing the following steps:\n",
    "\n",
    "Encoding categorical variables (creating dummy variables if needed).\n",
    "Splitting the data into training and testing sets.\n",
    "Standardizing or normalizing numerical features.\n",
    "Addressing any other preprocessing steps specific to the dataset.\n",
    "Example preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad543bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.get_dummies(data, columns=['categorical_column'])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c6bac",
   "metadata": {},
   "source": [
    "Step 4: Model Building\n",
    "Select and build a suitable regression model for predicting car prices. Justify your model choice and provide generic criteria for model selection. Consider linear regression, decision trees, random forests, and gradient boosting, among others.\n",
    "\n",
    "Example model building:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ad198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78491f80",
   "metadata": {},
   "source": [
    "Step 5: Model Evaluation\n",
    "Evaluate the model's performance using validation and test datasets. Calculate appropriate evaluation metrics (e.g., mean squared error, R-squared, etc.).\n",
    "\n",
    "Example model evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1937447",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ce5c19",
   "metadata": {},
   "source": [
    "Step 6: Lasso & Ridge Optimization\n",
    "Apply Lasso and Ridge regression to see if they improve the model's performance. Choose the regularization strength based on cross-validation.\n",
    "\n",
    "Example Lasso and Ridge optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a031d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lasso = Lasso()\n",
    "ridge = Ridge()\n",
    "\n",
    "# Create parameter grids for alpha values\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "lasso_cv = GridSearchCV(lasso, param_grid, cv=5)\n",
    "ridge_cv = GridSearchCV(ridge, param_grid, cv=5)\n",
    "\n",
    "# Fit Lasso and Ridge models\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "# Choose the best alpha values\n",
    "best_alpha_lasso = lasso_cv.best_params_['alpha']\n",
    "best_alpha_ridge = ridge_cv.best_params_['alpha']\n",
    "\n",
    "# Retrain models with best alpha values\n",
    "lasso = Lasso(alpha=best_alpha_lasso)\n",
    "ridge = Ridge(alpha=best_alpha_ridge)\n",
    "\n",
    "lasso.fit(X_train, y_train)\n",
    "ridge.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1382424",
   "metadata": {},
   "source": [
    "Step 7: Ensemble Models\n",
    "Build ensemble models like Random Forest and Gradient Boosting to see if they improve predictive performance.\n",
    "\n",
    "Example ensemble model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe0a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "rf_model = RandomForestRegressor()\n",
    "gb_model = GradientBoostingRegressor()\n",
    "rf_model.fit(X_train, y_train)\n",
    "gb_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22ce1f1",
   "metadata": {},
   "source": [
    "# Relevant Information\n",
    "\n",
    "1. Description:\n",
    "\n",
    "This data set consists of three types of entities:\n",
    "\n",
    "a. the specification of an auto in terms of various characteristics,\n",
    "\n",
    "b. Its assigned Insurance risk rating:\n",
    "\n",
    "This corresponds to the degree to which the auto is more risky than its price Indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process \"symboling\". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.\n",
    "\n",
    "c. Its normalized losses in use as compared to other cars:\n",
    "\n",
    "This factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc...), and represents the average loss per car per year.\n",
    "\n",
    "2. Missing values are denoted by \"NA\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488045c",
   "metadata": {},
   "source": [
    "#Answers\n",
    "\n",
    "\n",
    "Based on the provided information about the dataset, it's clear that you are dealing with a dataset containing information about cars, including their specifications, insurance risk rating, and normalized losses. Here's a summary of what you need to consider during your analysis:\n",
    "\n",
    "1. Data Description:\n",
    "\n",
    "a. Auto Specifications: This part of the dataset contains information about various characteristics of the cars, which will likely serve as the features for your predictive model. These characteristics may include attributes like make, model, horsepower, number of doors, engine type, fuel type, etc.\n",
    "\n",
    "b. Insurance Risk Rating (Symboling): The risk rating, represented by the \"symboling\" attribute, is a numerical value indicating the level of risk associated with each car. Positive values indicate higher risk, while negative values indicate lower risk.\n",
    "\n",
    "c. Normalized Losses: The \"normalized losses\" attribute represents the relative average loss payment per insured vehicle year, normalized for different size classifications of cars.\n",
    "\n",
    "2. Missing Values:\n",
    "\n",
    "The dataset contains missing values denoted by \"NA.\" You will need to handle these missing values as part of your data preprocessing.\n",
    "\n",
    "3. Model Building Criteria:\n",
    "\n",
    "When selecting a model for predicting car prices, you should consider the following criteria:\n",
    "\n",
    "The distribution of the target variable (car prices).\n",
    "The nature of the features (categorical, numerical, etc.).\n",
    "The potential presence of multicollinearity among features.\n",
    "Whether linear relationships are reasonable assumptions for the problem.\n",
    "The interpretability of the model.\n",
    "\n",
    "4. Lasso and Ridge Optimization:\n",
    "\n",
    "When performing Lasso and Ridge optimization, you'll want to explore whether adding regularization improves your model's predictive performance. Regularization can help prevent overfitting in cases where you have a large number of features.\n",
    "\n",
    "5. Ensemble Models:\n",
    "\n",
    "You can experiment with ensemble models like Random Forest and Gradient Boosting to see if they provide better predictive accuracy. These models are often robust and can capture non-linear relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f627001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
