{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c195f07",
   "metadata": {},
   "source": [
    "# 1. What is the purpose of text preprocessing in NLP, and why is it essential before analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1467bd",
   "metadata": {},
   "source": [
    "Text preprocessing is a crucial step in natural language processing (NLP) that involves cleaning and transforming raw text data into a format that is suitable for analysis and machine learning models. The purpose of text preprocessing is to enhance the quality of the input data and make it more manageable for further analysis. Here are some key reasons why text preprocessing is essential:\n",
    "\n",
    "Noise Reduction: Raw text data often contains noise, such as special characters, punctuation, and irrelevant symbols. Preprocessing helps remove or replace these elements, reducing noise and making the text cleaner.\n",
    "\n",
    "Tokenization: Text preprocessing involves breaking down the text into smaller units, called tokens. Tokens can be words, phrases, or even characters, depending on the level of granularity needed for analysis. Tokenization is a fundamental step in understanding the structure of the text.\n",
    "\n",
    "Normalization: Normalizing text involves converting all characters to lowercase, standardizing word forms (e.g., stemming or lemmatization), and handling variations in spelling or word usage. This ensures consistency in the representation of words and helps in capturing the core meaning.\n",
    "\n",
    "Stopword Removal: Stopwords are common words like \"and,\" \"the,\" and \"is\" that often do not contribute significant meaning to the text. Removing stopwords can help reduce dimensionality and improve the efficiency of subsequent analysis.\n",
    "\n",
    "Removing HTML Tags and URLs: If the text data is obtained from web sources, it may contain HTML tags or URLs. Preprocessing involves stripping these elements to focus on the actual textual content.\n",
    "\n",
    "Handling Missing Data: In real-world datasets, text data may have missing values or incomplete information. Text preprocessing may involve strategies such as imputation or removal of incomplete data to ensure the quality of the dataset.\n",
    "\n",
    "Vectorization: Machine learning models typically require numerical input. Text preprocessing includes converting textual data into numerical vectors using techniques such as one-hot encoding, TF-IDF (Term Frequency-Inverse Document Frequency), or word embeddings.\n",
    "\n",
    "Text Cleaning for Analysis: Depending on the analysis task, additional cleaning steps may be necessary, such as removing irrelevant information, handling special cases, or addressing specific issues related to the domain of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cfc931",
   "metadata": {},
   "source": [
    "# 2. Describe tokenization in NLP and explain its significance in text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b215d919",
   "metadata": {},
   "source": [
    "Tokenization is a fundamental process in natural language processing (NLP) that involves breaking down a text into smaller units, known as tokens. Tokens are the building blocks of language and can be words, phrases, sentences, or even characters, depending on the level of granularity needed for a specific task. The process of tokenization is crucial in text processing for several reasons:\n",
    "\n",
    "Text Understanding: Tokenization helps in understanding the structure of the text by breaking it into meaningful units. This is essential for various NLP tasks, as many algorithms and models operate on the level of individual words or phrases.\n",
    "\n",
    "Feature Extraction: In NLP, tokens serve as features that can be used for analysis or as input to machine learning models. Each token represents a discrete element of the text, and the combination of these tokens provides a numerical representation of the text that can be used for further analysis.\n",
    "\n",
    "Vocabulary Creation: Tokenization is a crucial step in building a vocabulary. The unique tokens present in a corpus form the vocabulary, which is important for tasks like text classification, sentiment analysis, and machine translation. A well-defined vocabulary is necessary for representing text data in a way that can be processed by algorithms.\n",
    "\n",
    "Statistical Analysis: Tokenization facilitates statistical analysis of text data. It allows for the calculation of metrics such as term frequency (TF) and inverse document frequency (IDF), which are essential in tasks like information retrieval and document ranking.\n",
    "\n",
    "Text Cleaning: Tokenization is often accompanied by text cleaning steps, where irrelevant elements like punctuation, special characters, and HTML tags are removed. Cleaning the text during tokenization helps create a more standardized and meaningful representation of the data.\n",
    "\n",
    "Language Processing Models: Many natural language processing models, such as language models and neural networks, operate at the level of tokens. Tokenization is a crucial step in preparing the input data for these models. It enables the model to process and learn patterns from the sequence of tokens in the text.\n",
    "\n",
    "Sentiment Analysis and Named Entity Recognition: Tasks like sentiment analysis and named entity recognition require an understanding of individual words or phrases. Tokenization provides the necessary units for analyzing sentiment in a sentence or identifying entities like names, locations, and organizations.\n",
    "\n",
    "There are different methods of tokenization, ranging from simple techniques like whitespace-based tokenization to more advanced approaches like using natural language processing libraries that can handle complexities such as compound words and contractions. The choice of tokenization method depends on the specific requirements of the NLP task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72005e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Sample text data\n",
    "text = \"Tokenization is an essential step in natural language processing.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenize_text(text)\n",
    "\n",
    "# Print the result\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2fa943",
   "metadata": {},
   "source": [
    "# 3. What are the differences between stemming and lemmatization in NLP? When would you choose one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea0f76d",
   "metadata": {},
   "source": [
    "Certainly! Stemming and lemmatization are both techniques used in natural language processing (NLP) to reduce words to their base or root forms, but they operate in slightly different ways. Here are the key differences between stemming and lemmatization:\n",
    "\n",
    "Stemming:\n",
    "Process:\n",
    "\n",
    "Definition: Stemming involves removing suffixes from words to obtain their root form.\n",
    "Goal: The goal is to reduce words to a common base form, even if the result is not a valid word.\n",
    "Output:\n",
    "\n",
    "Stemming may produce stems that are not actual words. For example, the stem of \"running\" would be \"run.\"\n",
    "Speed:\n",
    "\n",
    "Stemming is generally faster and computationally less expensive compared to lemmatization because it uses simpler rules and heuristics.\n",
    "Use Cases:\n",
    "\n",
    "Stemming is often used in information retrieval, search engines, and other applications where speed and efficiency are crucial.\n",
    "It is less concerned with maintaining the grammatical correctness of the resulting words.\n",
    "Lemmatization:\n",
    "Process:\n",
    "\n",
    "Definition: Lemmatization involves reducing words to their base or dictionary forms (lemmas), considering the context and meaning of the word.\n",
    "Goal: It typically requires a detailed understanding of the language and its morphology.\n",
    "Output:\n",
    "\n",
    "Lemmatization produces valid words, ensuring that the base form is a recognized word in the language. For example, the lemma of \"better\" would be \"good.\"\n",
    "Accuracy:\n",
    "\n",
    "Lemmatization is more accurate than stemming, as it considers the context and semantics of words. However, it is computationally more intensive than stemming.\n",
    "Use Cases:\n",
    "\n",
    "Lemmatization is preferred in applications where accuracy and semantic meaning are crucial, such as in natural language understanding, sentiment analysis, and machine translation.\n",
    "It is also useful when generating clean, grammatically correct text.\n",
    "When to Choose One Over the Other:\n",
    "Stemming:\n",
    "\n",
    "Use stemming when computational efficiency is a primary concern, and you can tolerate the possibility of getting word stems that are not valid words.\n",
    "Stemming is suitable for tasks like information retrieval, where speed is crucial.\n",
    "Lemmatization:\n",
    "\n",
    "Choose lemmatization when accuracy and semantic meaning are critical, and you need valid words in the output.\n",
    "Lemmatization is beneficial for tasks that require a deeper understanding of the language, such as text summarization or sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b01124",
   "metadata": {},
   "source": [
    "# 4. Explain the concept of stop words and their role in text preprocessing. How do they impact NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41fc65e",
   "metadata": {},
   "source": [
    "Role in Text Preprocessing:\n",
    "Dimensionality Reduction: One of the main roles of removing stop words in text preprocessing is to reduce the dimensionality of the data. By eliminating common words that are likely to appear in almost every document, the overall feature space becomes more manageable.\n",
    "\n",
    "Improved Processing Speed: Removing stop words can lead to faster processing of text data. Since stop words are ubiquitous and don't contribute much to the understanding of the content, excluding them can speed up various NLP tasks, such as text analysis, classification, and clustering.\n",
    "\n",
    "Focus on Content Words: By eliminating stop words, the emphasis is placed on content words that carry more significant meaning. This can enhance the accuracy of various NLP tasks by prioritizing words that contribute more to the overall semantics of the text.\n",
    "\n",
    "Enhanced Model Generalization: In machine learning models, removing stop words can lead to improved generalization. Stop words are often found in similar frequencies across different classes or categories, and removing them can help models focus on more meaningful words, potentially leading to better classification performance.\n",
    "\n",
    "Impact on NLP Tasks:\n",
    "Information Retrieval: Stop words are often removed in information retrieval tasks to improve the efficiency of search algorithms. Since stop words are common across many documents, they may not be useful for distinguishing between relevant and non-relevant documents.\n",
    "\n",
    "Text Classification: In tasks like sentiment analysis or text classification, stop words might not contribute much to determining the sentiment or category of a document. Removing them can lead to a more accurate representation of the content.\n",
    "\n",
    "Topic Modeling: In topic modeling applications, such as Latent Dirichlet Allocation (LDA), removing stop words is common. This helps in identifying the most meaningful words that contribute to defining topics within a corpus.\n",
    "\n",
    "Search Engines: Stop words are often ignored by search engines to improve query efficiency. For example, when searching for \"the best restaurants,\" the search engine might ignore the stop word \"the\" and focus on retrieving relevant documents containing \"best restaurants.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd2ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    filtered_text = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Reconstruct the text without stop words\n",
    "    text_without_stopwords = \" \".join(filtered_text)\n",
    "    \n",
    "    return text_without_stopwords\n",
    "\n",
    "# Sample text data\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Remove stop words\n",
    "text_without_stopwords = remove_stopwords(text)\n",
    "\n",
    "# Print the result\n",
    "print(text_without_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92057599",
   "metadata": {},
   "source": [
    "# 5. How does the process of removing punctuation contribute to text preprocessing in NLP? What are its benefits? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53603fa",
   "metadata": {},
   "source": [
    "The process of removing punctuation is an important step in text preprocessing in natural language processing (NLP). Punctuation removal involves eliminating characters such as periods, commas, exclamation marks, question marks, and other non-alphabetic symbols from text data. Here are some benefits of removing punctuation in NLP:\n",
    "\n",
    "Simplifies Tokenization:\n",
    "\n",
    "Removing punctuation simplifies the tokenization process. Tokenization involves breaking down text into smaller units, and removing punctuation ensures that words are cleanly separated into tokens. This is crucial for accurately representing the structure of the text.\n",
    "Consistent Word Representations:\n",
    "\n",
    "Punctuation removal contributes to the creation of consistent word representations. Words with or without punctuation may be treated as distinct tokens by models, leading to variations in representation. By removing punctuation, the text data becomes more standardized.\n",
    "Reduces Dimensionality:\n",
    "\n",
    "Punctuation marks often do not carry significant meaning on their own and can contribute to the dimensionality of the data without providing much value. Removing punctuation helps reduce the feature space, making it more manageable for analysis and modeling.\n",
    "Improves Text Cleaning:\n",
    "\n",
    "Punctuation removal is a part of the overall text cleaning process. Clean text data is essential for accurate analysis and modeling. Eliminating punctuation contributes to creating a more refined and readable text, which is especially important for tasks like sentiment analysis and language modeling.\n",
    "Enhances Model Performance:\n",
    "\n",
    "Many natural language processing models, especially those based on bag-of-words representations, can benefit from the removal of punctuation. Punctuation marks may interfere with the model's ability to recognize and generalize patterns, so their removal can contribute to improved model performance.\n",
    "Facilitates NLP Tasks:\n",
    "\n",
    "Punctuation removal is crucial for various NLP tasks, such as part-of-speech tagging, named entity recognition, and sentiment analysis. These tasks often rely on the correct identification and classification of words, which can be impeded by the presence of punctuation.\n",
    "Supports Language Models:\n",
    "\n",
    "Language models, including neural network-based models, often process text at the word level. Punctuation removal helps create a more consistent and standardized input for these models, allowing them to better capture contextual relationships between words.\n",
    "Improves Text Similarity Measures:\n",
    "\n",
    "In tasks related to text similarity or distance measures, removing punctuation ensures that the comparison is based on the actual content of the text rather than on variations caused by punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c497e3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world How are you\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Use string.punctuation to get a string of all punctuation marks\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    # Use translate to remove punctuation from the text\n",
    "    text_without_punctuation = text.translate(translator)\n",
    "    \n",
    "    return text_without_punctuation\n",
    "\n",
    "# Sample text data\n",
    "text = \"Hello, world! How are you?\"\n",
    "\n",
    "# Remove punctuation\n",
    "text_without_punctuation = remove_punctuation(text)\n",
    "\n",
    "# Print the result\n",
    "print(text_without_punctuation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb392ef9",
   "metadata": {},
   "source": [
    "# 6. Discuss the importance of lowercase conversion in text preprocessing. Why is it a common step in NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355074bd",
   "metadata": {},
   "source": [
    "Lowercase conversion is a common step in text preprocessing for various natural language processing (NLP) tasks. The importance of converting text to lowercase lies in achieving consistency, reducing dimensionality, and ensuring that the same word is treated consistently across different contexts. Let's discuss the reasons and provide a Python code example:\n",
    "\n",
    "Importance of Lowercase Conversion:\n",
    "Uniformity in Text Representation:\n",
    "\n",
    "Converting all text to lowercase ensures uniformity in text representation. This is crucial for tasks that involve comparing or matching words, as it prevents the same word with different cases from being treated differently.\n",
    "Consistent Tokenization:\n",
    "\n",
    "Tokenization, the process of breaking down text into individual words or tokens, becomes more consistent when text is in lowercase. It ensures that the same word, regardless of its case, is treated as the same token.\n",
    "Improved Generalization:\n",
    "\n",
    "Lowercasing contributes to improved generalization in machine learning models. Models can learn more effectively when words are treated consistently, allowing them to better capture patterns and relationships in the data.\n",
    "Efficient Text Matching:\n",
    "\n",
    "Lowercasing is essential for tasks involving text matching and comparison, such as deduplication, information retrieval, and search engines. It ensures that matches are not missed due to variations in case.\n",
    "Reduced Dimensionality:\n",
    "\n",
    "Lowercasing helps reduce the dimensionality of the feature space. Without lowercasing, the same word with different cases may be treated as distinct features, leading to increased dimensionality and potential sparsity in the data.\n",
    "Consistent Semantics:\n",
    "\n",
    "In many cases, the semantics of words are not affected by case. Lowercasing preserves the semantic meaning of words while simplifying their representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25bb61fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox jumps over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "# Sample text data\n",
    "text = \"The Quick BROWN Fox JUMPS Over the Lazy DOG\"\n",
    "\n",
    "# Convert text to lowercase\n",
    "lowercased_text = text.lower()\n",
    "\n",
    "# Print the result\n",
    "print(lowercased_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd254e",
   "metadata": {},
   "source": [
    "# 7. Explain the term \"vectorization\" concerning text data. How does techniques like CountVectorizer contribute to text preprocessing in NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0856ce44",
   "metadata": {},
   "source": [
    "Vectorization in the context of text data involves converting textual information into numerical vectors, making it suitable for machine learning models. In NLP, one common technique for vectorization is using the CountVectorizer, which represents each document as a vector of word frequencies. Let's illustrate this concept using Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ffe4d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  document  first  is  one  second  the  third  this\n",
      "0    0         1      1   1    0       0    1      0     1\n",
      "1    0         2      0   1    0       1    1      0     1\n",
      "2    1         0      0   1    1       0    1      1     1\n",
      "3    0         1      1   1    0       0    1      0     1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text data\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the sparse matrix to a dense array for better readability\n",
    "dense_array = X.toarray()\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(dense_array, columns=feature_names)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fef88f",
   "metadata": {},
   "source": [
    "In this example, the CountVectorizer performs the following:\n",
    "\n",
    "Tokenization: Splits each document into individual words or terms.\n",
    "Building Vocabulary: Constructs a vocabulary of unique terms present in the entire corpus.\n",
    "Vectorization: Represents each document as a vector of word frequencies, where each element in the vector corresponds to the count of a specific word in the document.\n",
    "The resulting matrix (sparse in this case) represents the vectorized form of the text data. Each row corresponds to a document, and each column corresponds to a unique word in the vocabulary. The values in the matrix indicate the frequency of each word in the respective documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21616a7c",
   "metadata": {},
   "source": [
    "# 8. Describe the concept of normalization in NLP. Provide examples of normalization techniques used in text preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b7e641",
   "metadata": {},
   "source": [
    "Normalization in natural language processing (NLP) involves transforming text data into a standardized format to make it more consistent and suitable for analysis. Let's explore some common normalization techniques using Python code:\n",
    "\n",
    "1. Lowercasing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9223018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox jumps over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "text = \"The Quick BROWN Fox JUMPS Over the Lazy DOG\"\n",
    "normalized_text = text.lower()\n",
    "print(normalized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b7467",
   "metadata": {},
   "source": [
    "2. Removing Punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "729a63d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world How are you\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text = \"Hello, world! How are you?\"\n",
    "translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "normalized_text = text.translate(translator)\n",
    "print(normalized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe5128b",
   "metadata": {},
   "source": [
    "3. Removing Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaccf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "filtered_text = [word for word in tokens if word.lower() not in stop_words]\n",
    "normalized_text = \" \".join(filtered_text)\n",
    "print(normalized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c550b1",
   "metadata": {},
   "source": [
    "4. Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac36a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text = \"running ran runs\"\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "lemmatized_text = \" \".join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "print(lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aede04",
   "metadata": {},
   "source": [
    "5. Stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e9ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "text = \"running ran runs\"\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "stemmed_text = \" \".join([stemmer.stem(word) for word in tokens])\n",
    "print(stemmed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2540936f",
   "metadata": {},
   "source": [
    "6. Numeric and Alphanumeric Token Removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb3604e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meeting at 3pm. Room A123.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Meeting at 3pm. Room A123.\"\n",
    "normalized_text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "print(normalized_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
