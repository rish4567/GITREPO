{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "832cb774",
   "metadata": {},
   "source": [
    "# 1. Build a Transfer Learning image classification model using the VGG16 & VGG19(pre-training network)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace39e33",
   "metadata": {},
   "source": [
    "\n",
    "Certainly! Transfer learning involves using a pre-trained model on a large dataset and adapting it to a new task. In this case, we'll use the VGG16 and VGG19 models, which are popular deep learning models for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0632b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16, VGG19\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = (224, 224)  # VGG models require input images to be 224x224 pixels\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "NUM_CLASSES = 10  # Number of output classes for your specific task\n",
    "\n",
    "# Load the pre-trained VGG16 model with weights pre-trained on ImageNet\n",
    "base_model_vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "# Alternatively, you can use VGG19\n",
    "# base_model_vgg19 = VGG19(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "# Build your model on top of the pre-trained VGG16 model\n",
    "model = Sequential()\n",
    "model.add(base_model_vgg16)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "# Freeze the layers of the pre-trained VGG16 model\n",
    "for layer in base_model_vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656e42ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Replace 'path/to/train' with the path to your training dataset. Adjust other parameters based on your specific needs.\n",
    "\n",
    "Making sure to have a directory structure like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db66eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "- path/to/train\n",
    "  - class_1\n",
    "    - image1.jpg\n",
    "    - image2.jpg\n",
    "    - ...\n",
    "  - class_2\n",
    "    - image1.jpg\n",
    "    - image2.jpg\n",
    "    - ...\n",
    "  - ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b052fa8",
   "metadata": {},
   "source": [
    "# 2. Build a Multiclass image classification model with inceptionV3 and Mobilenet pretrained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b73feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionV3, MobileNet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = (224, 224)  # Both InceptionV3 and MobileNet require input images to be 224x224 pixels\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "NUM_CLASSES = 10  # Number of output classes for your specific task\n",
    "\n",
    "# Load the pre-trained InceptionV3 model with weights pre-trained on ImageNet\n",
    "base_model_inception = InceptionV3(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "# Alternatively, you can use MobileNet\n",
    "# base_model_mobilenet = MobileNet(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "# Build your model on top of the pre-trained InceptionV3 model\n",
    "model = Sequential()\n",
    "model.add(base_model_inception)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "# Freeze the layers of the pre-trained InceptionV3 model\n",
    "for layer in base_model_inception.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08133b9b",
   "metadata": {},
   "source": [
    "Replace 'path/to/train' with the path to your training dataset. Adjust other parameters based on your specific needs.\n",
    "\n",
    "Making sure to have a directory structure like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18315a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "- path/to/train\n",
    "  - class_1\n",
    "    - image1.jpg\n",
    "    - image2.jpg\n",
    "    - ...\n",
    "  - class_2\n",
    "    - image1.jpg\n",
    "    - image2.jpg\n",
    "    - ...\n",
    "  - ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
