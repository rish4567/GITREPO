{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d57337b",
   "metadata": {},
   "source": [
    "# Predicting Housing Prices with Regularized Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff091c55",
   "metadata": {},
   "source": [
    "# You work for a real estate analytics firm, and your task is to build a predictive model to estimate house prices based on various features. You have a dataset containing information about houses, such as square tamage, number of bedrooms, number of bathrooms, and other relevant attributes, in this case study. you'll explore the application of Lange regression to improve the predictive performance of the model:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b6c30",
   "metadata": {},
   "source": [
    "# 1. Data Preparation:\n",
    "\n",
    "a. Load the dataset using pandas\n",
    "\n",
    "b. Explore and clean the data. Handle missing values and outliers\n",
    "\n",
    "c. Spilt the dataset into training and testing sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5667be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas numpy scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2286f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('house_prices_dataset.csv')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4af54e",
   "metadata": {},
   "source": [
    "b. Explore and clean the data:\n",
    "\n",
    "Data exploration is an important step to understand your dataset. You should check for missing values, outliers, and understand the data distribution. Here are some common data exploration and cleaning tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6641002",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)\n",
    "data = data.dropna()\n",
    "data_description = data.describe()\n",
    "print(data_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0c4d6d",
   "metadata": {},
   "source": [
    "c. Split the dataset into training and testing sets:\n",
    "\n",
    "To build and evaluate your predictive model, you need to split the dataset into training and testing sets. Typically, an 80-20 or 70-30 split is used, where the larger portion is used for training, and the smaller portion is used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e54168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data.drop(columns=['Price']) \n",
    "y = data['Price']  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f57c8",
   "metadata": {},
   "source": [
    "# 2. Implement Lasso Regression:\n",
    "\n",
    "a. Choose a set of features (independent variables, X) and house prices as the dependent variable (y)\n",
    "\n",
    "b. Implement Lasso regression using scikit-learn to predict house prices based on the selected features\n",
    "\n",
    "c. Discuss the impact of L1 regularization on feature selection and coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff1761",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['SquareFootage', 'Bedrooms', 'Bathrooms', 'YearBuilt', 'GarageSize']\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a53be90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_model = Lasso(alpha=1.0) \n",
    "lasso_model.fit(X_train_selected, y_train)\n",
    "y_pred = lasso_model.predict(X_test_selected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201215c5",
   "metadata": {},
   "source": [
    "c. Discuss the impact of L1 regularization on feature selection and coefficients:\n",
    "\n",
    "L1 regularization (Lasso) has a significant impact on feature selection and the coefficients of the model. It encourages sparsity in the model, meaning it tends to drive some coefficients to exactly zero, effectively removing those features from the model. Here are some key points to consider:\n",
    "\n",
    "Feature Selection: Lasso automatically selects a subset of the most important features while setting the coefficients of less important features to zero. This is very useful for models with many features, as it simplifies the model and potentially reduces overfitting.\n",
    "\n",
    "Coefficient Shrinking: L1 regularization also \"shrinks\" the coefficients of the selected features towards zero. The degree of shrinking is controlled by the regularization strength (alpha parameter). Larger values of alpha result in stronger regularization and more coefficients being driven to zero.\n",
    "\n",
    "Interpretability: Lasso's feature selection property makes the model more interpretable. You can easily identify the most influential features by looking at the non-zero coefficients.\n",
    "\n",
    "Trade-off: The choice of the alpha parameter is a trade-off between fitting the data well (low bias) and preventing overfitting (low variance). You may need to tune the alpha value through techniques like cross-validation to find the best balance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b11ce00",
   "metadata": {},
   "source": [
    "# 3. Evaluate the Lasso Regression Model:\n",
    "\n",
    "a. Calculate the Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) for the Lasso regression model.\n",
    "\n",
    "b. Discuss how the Lasso model helps prevent overfitting and reduces the impact of irrelevant features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1094f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "y_pred = lasso_model.predict(X_test_selected)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aebb7ff",
   "metadata": {},
   "source": [
    "b. Discuss how the Lasso model helps prevent overfitting and reduces the impact of irrelevant features:\n",
    "\n",
    "Lasso Regression is particularly effective at preventing overfitting and reducing the impact of irrelevant features due to its L1 regularization property. Here's how it accomplishes these goals:\n",
    "\n",
    "Feature Selection: Lasso automatically selects a subset of the most important features by driving the coefficients of less important features to zero. This feature selection mechanism simplifies the model by excluding irrelevant features, reducing its complexity. As a result, the model is less prone to overfitting because it's not trying to fit noise from irrelevant features.\n",
    "\n",
    "Regularization: L1 regularization (Lasso) adds a penalty term to the linear regression cost function, which encourages the absolute values of the coefficients to be small. This has the effect of regularizing the model and preventing it from fitting the training data too closely. In other words, it reduces the model's complexity and capacity, which is a key factor in preventing overfitting.\n",
    "\n",
    "Interpretability: Lasso's feature selection property improves the interpretability of the model. You can easily identify which features are important (those with non-zero coefficients) and which are not. This can help in understanding the factors that influence house prices and in making data-driven decisions.\n",
    "\n",
    "Hyperparameter Tuning: You can control the strength of L1 regularization through the alpha hyperparameter. By tuning this hyperparameter, you can find the right balance between fitting the data well and regularization. Larger values of alpha increase the regularization effect, which can be useful for controlling overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd0eb3",
   "metadata": {},
   "source": [
    "# 4. Implement Ridge Regression:\n",
    "\n",
    "a. Select the same set of features as independent variables (X) and house prices as the dependent variable (v).\n",
    "\n",
    "b. Implement Ridge regression using scikit-learn to predict house prices based on the selected features\n",
    "\n",
    "c. Explain how 12 regularization in Ridge regression differs from L1 regularization in Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83dfd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['SquareFootage', 'Bedrooms', 'Bathrooms', 'YearBuilt', 'GarageSize']\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef3e512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_model = Ridge(alpha=1.0) \n",
    "ridge_model.fit(X_train_selected, y_train)\n",
    "y_pred_ridge = ridge_model.predict(X_test_selected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb20089",
   "metadata": {},
   "source": [
    "c. Explain how L2 regularization in Ridge differs from L1 regularization in Lasso:\n",
    "\n",
    "L2 regularization in Ridge and L1 regularization in Lasso are two common techniques used to add regularization to linear regression models. They differ in the way they penalize the coefficients of the features:\n",
    "\n",
    "L1 Regularization (Lasso):\n",
    "\n",
    "Encourages sparsity by driving some coefficients to exactly zero.\n",
    "Results in feature selection, as it automatically selects a subset of important features while excluding irrelevant ones.\n",
    "L1 regularization is less prone to multicollinearity issues (when features are highly correlated), as it tends to select one feature from a group of correlated features.\n",
    "It's more interpretable because it explicitly sets some coefficients to zero, making it clear which features are not contributing to the model.\n",
    "L2 Regularization (Ridge):\n",
    "\n",
    "Penalizes the sum of the squared values of the coefficients without driving any coefficients to exactly zero.\n",
    "Reduces the magnitude of all coefficients, but none become exactly zero.\n",
    "L2 regularization is effective in reducing the impact of multicollinearity by spreading the impact of correlated features across all of them.\n",
    "It can lead to a more stable and numerically well-conditioned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35068330",
   "metadata": {},
   "source": [
    "# 5. Evaluate the Ridge Regression Model:\n",
    "\n",
    "a Calculate the MAE, MSE, and RMSE for the Ridge regression model.\n",
    "\n",
    "b. Discuss the benefits of Ridge regression in handling multicollinearity among features and is impact on the model's coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96714c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "y_pred_ridge = ridge_model.predict(X_test_selected)\n",
    "mae_ridge = mean_absolute_error(y_test, y_pred_ridge)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "rmse_ridge = np.sqrt(mse_ridge)\n",
    "print(\"Ridge Regression Metrics:\")\n",
    "print(\"Mean Absolute Error (MAE):\", mae_ridge)\n",
    "print(\"Mean Squared Error (MSE):\", mse_ridge)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse_ridge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071183f",
   "metadata": {},
   "source": [
    "b. Benefits of Ridge Regression in Handling Multicollinearity:\n",
    "\n",
    "Ridge Regression is effective in handling multicollinearity, which occurs when independent variables (features) in the dataset are highly correlated with each other. Multicollinearity can make it challenging to interpret the individual impact of each feature on the target variable.\n",
    "In Ridge Regression, L2 regularization adds a penalty to the sum of the squared coefficients, which has the effect of reducing the magnitude of all coefficients. This means that it doesn't eliminate any feature but spreads the impact of correlated features across all of them.\n",
    "By reducing the magnitude of coefficients uniformly, Ridge Regression helps to balance the contribution of correlated features, making the model more robust to multicollinearity.\n",
    "Impact on the Model's Coefficients:\n",
    "\n",
    "Ridge Regression does not drive any coefficients to zero; all features are retained in the model.\n",
    "The coefficients in Ridge Regression are \"shrunken\" towards zero but remain non-zero. This means that all features are considered to some extent in predicting the target variable.\n",
    "The regularization strength (alpha) in Ridge can be adjusted to control the degree of coefficient shrinkage. Larger alpha values lead to stronger regularization and more coefficient shrinkage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43637fe",
   "metadata": {},
   "source": [
    "# 6. Model Comparison:\n",
    "\n",
    "a. Compare the results of the Lasso and Ridge regression models.\n",
    "\n",
    "b. Discuss when it is preferable to use Lasso, Ridge, or plain linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c406c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results of Lasso Regression\n",
    "print(\"Lasso Regression Metrics:\")\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "# Results of Ridge Regression\n",
    "print(\"Ridge Regression Metrics:\")\n",
    "print(\"Mean Absolute Error (MAE):\", mae_ridge)\n",
    "print(\"Mean Squared Error (MSE):\", mse_ridge)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse_ridge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f2616",
   "metadata": {},
   "source": [
    "b. Discuss when it is preferable to use Lasso, Ridge, or plain linear regression:\n",
    "\n",
    "Linear Regression (Ordinary Least Squares - OLS):\n",
    "\n",
    "Use plain linear regression when you have no concerns about overfitting and multicollinearity.\n",
    "It's suitable when you have a small number of features and you want a simple model that interprets each feature's direct impact on the target variable.\n",
    "Lasso Regression (L1 Regularization):\n",
    "\n",
    "Use Lasso when you want automatic feature selection and sparsity in the model.\n",
    "It's preferable when you have many features, some of which may be irrelevant or highly correlated, and you want to reduce overfitting by driving some coefficients to zero.\n",
    "Lasso is valuable when model interpretability and feature importance are essential.\n",
    "Ridge Regression (L2 Regularization):\n",
    "\n",
    "Use Ridge when you want to handle multicollinearity among features and improve the stability of your model.\n",
    "It's preferable when all the features are relevant, and you want to prevent overfitting by shrinking the coefficients uniformly.\n",
    "Ridge helps maintain all features in the model, and it can be particularly useful when you have a large number of correlated features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2985ed2d",
   "metadata": {},
   "source": [
    "# 7. Hyperparameter Tuning:\n",
    "\n",
    "a. Explore hyperparameter tuning for Lasso and Ridge, such as the strength of regularization, and discuss how different hyperparameters affect the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d532af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create Lasso and Ridge models\n",
    "lasso_model = Lasso()\n",
    "ridge_model = Ridge()\n",
    "\n",
    "# Define a range of alpha values to explore\n",
    "alphas = [0.01, 0.1, 1, 10, 100]  # You can extend this list\n",
    "\n",
    "# Create parameter grids for alpha values\n",
    "param_grid_lasso = {'alpha': alphas}\n",
    "param_grid_ridge = {'alpha': alphas}\n",
    "\n",
    "# Perform grid search for Lasso\n",
    "lasso_grid = GridSearchCV(lasso_model, param_grid_lasso, scoring='neg_mean_squared_error', cv=5)\n",
    "lasso_grid.fit(X_train_selected, y_train)\n",
    "\n",
    "# Perform grid search for Ridge\n",
    "ridge_grid = GridSearchCV(ridge_model, param_grid_ridge, scoring='neg_mean_squared_error', cv=5)\n",
    "ridge_grid.fit(X_train_selected, y_train)\n",
    "\n",
    "# Best alpha values for Lasso and Ridge\n",
    "best_alpha_lasso = lasso_grid.best_params_['alpha']\n",
    "best_alpha_ridge = ridge_grid.best_params_['alpha']\n",
    "\n",
    "# Create Lasso and Ridge models with the best alpha values\n",
    "best_lasso_model = Lasso(alpha=best_alpha_lasso)\n",
    "best_ridge_model = Ridge(alpha=best_alpha_ridge)\n",
    "\n",
    "# Fit the models with the best alpha values\n",
    "best_lasso_model.fit(X_train_selected, y_train)\n",
    "best_ridge_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the models with the best alpha values on the test data\n",
    "y_pred_best_lasso = best_lasso_model.predict(X_test_selected)\n",
    "y_pred_best_ridge = best_ridge_model.predict(X_test_selected)\n",
    "\n",
    "# Calculate evaluation metrics for the best models\n",
    "mae_best_lasso = mean_absolute_error(y_test, y_pred_best_lasso)\n",
    "mae_best_ridge = mean_absolute_error(y_test, y_pred_best_ridge)\n",
    "# Calculate other metrics (MSE, RMSE) as well\n",
    "\n",
    "print(\"Best Alpha for Lasso:\", best_alpha_lasso)\n",
    "print(\"Best Alpha for Ridge:\", best_alpha_ridge)\n",
    "print(\"MAE for Lasso with Best Alpha:\", mae_best_lasso)\n",
    "print(\"MAE for Ridge with Best Alpha:\", mae_best_ridge)\n",
    "# Print other metrics as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a758a135",
   "metadata": {},
   "source": [
    "# 8. Model Improvement:\n",
    "\n",
    "a. Investigate any feature engineering or data preprocessing techniques that can enhance the performance of the regularized regression models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b43dab",
   "metadata": {},
   "source": [
    "\n",
    "Improving the performance of regularized regression models, such as Lasso and Ridge, can be achieved through various feature engineering and data preprocessing techniques. Here are some strategies to enhance the performance of these models:\n",
    "\n",
    "Feature Scaling:\n",
    "\n",
    "Standardize or normalize the feature variables. Standardization (mean=0, std=1) is essential for some regularization techniques like Ridge. Normalization (scaling to a range like [0, 1]) can be useful when feature scales vary significantly.\n",
    "Polynomial Features:\n",
    "\n",
    "Consider adding polynomial features, such as squared or cubic terms of the existing features, to capture non-linear relationships. You can do this using scikit-learn's PolynomialFeatures class.\n",
    "Feature Interaction:\n",
    "\n",
    "Create new features that represent interactions between two or more existing features. For example, if you have 'Bedrooms' and 'Bathrooms,' you can create an 'Interaction' feature that is the product of the two.\n",
    "One-Hot Encoding:\n",
    "\n",
    "For categorical variables, use one-hot encoding to transform them into binary columns. This is crucial for Lasso and Ridge, as they rely on numerical features.\n",
    "Outlier Handling:\n",
    "\n",
    "Identify and handle outliers in the dataset, as they can significantly impact model performance. You can remove outliers, transform them, or use robust regression techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b5f59e",
   "metadata": {},
   "source": [
    "# 9. Conclusion:\n",
    "\n",
    "a. Summarize the findings and provide insights into how Lasso and Ridge regression can be valuable tools for estimating house prices and handling complex datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f7954",
   "metadata": {},
   "source": [
    "In conclusion, Lasso and Ridge Regression are valuable tools for estimating house prices and handling complex datasets. Here are the key findings and insights:\n",
    "\n",
    "Regularization and Feature Selection: Lasso Regression is particularly effective in feature selection, automatically driving some coefficients to zero and providing a sparse model. This is valuable for handling datasets with many features, helping to identify and focus on the most relevant ones.\n",
    "\n",
    "Multicollinearity: Ridge Regression is a powerful tool for handling multicollinearity among features. It redistributes the impact of correlated features and stabilizes the model, making it suitable for datasets with highly correlated variables.\n",
    "\n",
    "Model Interpretability: Lasso and Ridge provide model interpretability. Lasso explicitly selects important features, making it easier to understand the factors influencing house prices. Ridge retains all features to some extent, providing a more comprehensive view.\n",
    "\n",
    "Regularization Strength: The choice of regularization strength (alpha) is essential. It can be adjusted to find the right balance between model complexity and fitting the data well. Cross-validation is valuable for determining the optimal alpha.\n",
    "\n",
    "Model Improvement: Feature engineering, data preprocessing, and domain-specific knowledge play a significant role in enhancing the performance of regularized regression models. Techniques such as feature scaling, interaction features, one-hot encoding, and outlier handling can significantly improve model accuracy.\n",
    "\n",
    "Ensemble Learning: Combining the strengths of regularized regression models with other techniques, such as tree-based models or ensembles, can yield even better results.\n",
    "\n",
    "Cross-Validation: Implementing cross-validation is crucial for robust model assessment and selection of the best hyperparameters.\n",
    "\n",
    "Domain Knowledge: Incorporating domain-specific knowledge can lead to more meaningful feature engineering and more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1f96b",
   "metadata": {},
   "source": [
    "# Diagnosing and Remedying Heteroscedasticity and Multicollinearity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be78da9",
   "metadata": {},
   "source": [
    "\n",
    "You are working as a data analyst for a company that aims to predict employee performance based on various factors such as experience, education level, and the number of projects completed. You've built a linear regression model, but you suspect it may be suffering from issues related to heteroscedasticity and multicollinearity. Your task is to diagnose and address these problems:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7feac1",
   "metadata": {},
   "source": [
    "# 1. Initial Linear Regression Model:\n",
    "\n",
    "a. Describe the dataset and the variables you're using for predicting employee performance.\n",
    "\n",
    "b. Implement a simple linear regression model to predict employee performance.\n",
    "\n",
    "c. Discuss why linear regression is a suitable choice for this prediction problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e891fc8c",
   "metadata": {},
   "source": [
    "a. Describe the dataset and the variables for predicting employee performance:\n",
    "\n",
    "The dataset used for predicting employee performance contains various factors related to employees' characteristics and job-related attributes. The dataset includes the following variables:\n",
    "\n",
    "Experience: The number of years of work experience the employee has.\n",
    "Education Level: The highest level of education the employee has completed (e.g., high school diploma, bachelor's degree, master's degree, etc.).\n",
    "Number of Projects Completed: The total number of projects an employee has successfully completed.\n",
    "Employee Performance: The target variable representing the employee's performance, typically measured using a numerical scale or a performance score.\n",
    "The dataset aims to determine how experience, education level, and the number of projects completed influence an employee's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff718e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset (assuming 'data' is the DataFrame)\n",
    "# For this example, we will use 'Experience' to predict 'Employee Performance'\n",
    "X = data[['Experience']]\n",
    "y = data['Employee Performance']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit a simple linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c2905e",
   "metadata": {},
   "source": [
    "c. Discuss why linear regression is a suitable choice for this prediction problem:\n",
    "\n",
    "Linear regression is a suitable choice for predicting employee performance in this context for the following reasons:\n",
    "\n",
    "Linearity Assumption: Linear regression assumes a linear relationship between the predictor variables (experience, education level, number of projects) and the target variable (employee performance). This assumption is reasonable for many real-world scenarios, especially when the relationship between variables is expected to be roughly linear.\n",
    "\n",
    "Interpretability: Linear regression models are highly interpretable. The coefficients of the model provide insight into how each predictor variable affects the target variable. This interpretability can be valuable for understanding the factors that influence employee performance.\n",
    "\n",
    "Ease of Implementation: Linear regression is straightforward to implement and does not require complex algorithms or hyperparameter tuning, making it a practical choice for initial modeling.\n",
    "\n",
    "Baseline Model: It serves as a useful baseline model to establish a benchmark for predictive performance. If a linear regression model provides satisfactory results, there may be no need for more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de7e793",
   "metadata": {},
   "source": [
    "# 2. Identifying Heteroscedasticity:\n",
    "\n",
    "a. Explain what heteroscedasticity is in the context of linear regression.\n",
    "\n",
    "b. Provide methods for diagnosing heteroscedasticity in a regression model.\n",
    "\n",
    "c. Apply these diagnostic methods to your model's residuals and report your findings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762b1bf1",
   "metadata": {},
   "source": [
    "a. Explain what heteroscedasticity is in the context of linear regression:\n",
    "\n",
    "Heteroscedasticity, in the context of linear regression, refers to a situation where the variance of the residuals (the differences between the observed values and the predicted values) is not constant across all levels of the independent variables. In simpler terms, it means that the spread or dispersion of the residuals varies as the independent variable(s) change. This violates one of the key assumptions of linear regression, which assumes that the variance of the residuals should be constant (homoscedastic) across all values of the predictors.\n",
    "\n",
    "In other words, heteroscedasticity indicates that the errors have different levels of variability for different values of the predictor variables, and this can lead to problems in model interpretation and inference. It can affect the model's predictive accuracy and make it less reliable."
   ]
  },
  {
   "cell_type": "raw",
   "id": "32bb68bb",
   "metadata": {},
   "source": [
    "b. Provide methods for diagnosing heteroscedasticity in a regression model:\n",
    "\n",
    "Diagnosing heteroscedasticity typically involves examining plots and statistical tests. Here are common methods for diagnosing heteroscedasticity:\n",
    "\n",
    "Residual Plots: Plot the residuals against the predicted values (or one of the independent variables). If you observe a fan-shaped pattern or a funnel-like shape in the plot, it may be an indicator of heteroscedasticity.\n",
    "\n",
    "Scale-Location Plot: Create a plot of the square root of the standardized residuals against the predicted values. Heteroscedasticity is indicated if you see a clear pattern or increasing spread in this plot.\n",
    "\n",
    "Breusch-Pagan Test: This is a statistical test to formally test for heteroscedasticity. The null hypothesis is that the variance of the residuals is constant (homoscedastic), while the alternative hypothesis is that the variance is not constant (heteroscedastic).\n",
    "\n",
    "White Test: The White test is another statistical test to detect heteroscedasticity. It's a more general test that can identify both heteroscedasticity and serial correlation in the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4700176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have your model and residuals (e.g., model and residuals)\n",
    "\n",
    "# Residual plot\n",
    "plt.scatter(model.predict(X_train), residuals)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()\n",
    "\n",
    "# Scale-Location Plot\n",
    "sqrt_standardized_residuals = np.sqrt(np.abs(residuals) / residuals.std())\n",
    "plt.scatter(model.predict(X_train), sqrt_standardized_residuals)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('√|Standardized Residuals|')\n",
    "plt.title('Scale-Location Plot')\n",
    "plt.show()\n",
    "\n",
    "# Breusch-Pagan Test\n",
    "name = ['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value']\n",
    "test = sms.het_breuschpagan(residuals, X_train)\n",
    "print(dict(zip(name, test)))\n",
    "\n",
    "# White Test\n",
    "name = ['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value']\n",
    "test = sms.het_white(residuals, X_train)\n",
    "print(dict(zip(name, test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8647bbf",
   "metadata": {},
   "source": [
    "# 3. Remedying Heteroscedasticity:\n",
    "\n",
    "a. Discuss the potential consequences of heteroscedasticity on your regression model.\n",
    "\n",
    "b. Suggest ways to address heteroscedasticity, such as transforming variables or using weighted least squares regression\n",
    "\n",
    "c. Implement the recommended remedial actions and evaluate their impact on the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26997e",
   "metadata": {},
   "source": [
    "a. Discuss the potential consequences of heteroscedasticity on your regression model:\n",
    "\n",
    "Heteroscedasticity can have several adverse consequences on a regression model:\n",
    "\n",
    "Biased Coefficients: Heteroscedasticity can lead to biased coefficient estimates because the model assigns more weight to observations with larger residuals (higher variability), which can distort the relationships between variables.\n",
    "\n",
    "Inefficient Estimators: Ordinary Least Squares (OLS) assumes constant variance of residuals, and when this assumption is violated, OLS estimators are no longer BLUE (Best Linear Unbiased Estimators). They become inefficient and have larger standard errors.\n",
    "\n",
    "Incorrect Inference: Heteroscedasticity can lead to incorrect statistical inference, such as overestimating the significance of predictors, as p-values and confidence intervals may not accurately reflect the true level of uncertainty in parameter estimates.\n",
    "\n",
    "Reduced Predictive Accuracy: Heteroscedasticity can negatively affect the model's predictive accuracy because it assigns too much influence to outliers or high-variance observations, leading to less reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5899b4d5",
   "metadata": {},
   "source": [
    "# 4. Detecting Multicollinearity:\n",
    "\n",
    "a. Explain what multicollinearity is and how it can affect a linear regression model.\n",
    "\n",
    "b. Use correlation matrices or variance inflation factors (VIFS) to identify multicollinearity in your predictor variables.\n",
    "\n",
    "c. Present your findings regarding which variables are highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9516641b",
   "metadata": {},
   "source": [
    "a. Explain what multicollinearity is and how it can affect a linear regression model:\n",
    "\n",
    "Multicollinearity is a statistical phenomenon in which two or more independent variables in a regression model are highly correlated with each other. This high correlation among predictor variables can have several negative effects on a linear regression model:\n",
    "\n",
    "Inflated Standard Errors: Multicollinearity can lead to inflated standard errors of the regression coefficients. This means that the estimated coefficients' precision is reduced, making it challenging to assess the individual effects of predictors.\n",
    "\n",
    "Unstable Coefficients: Small changes in the data can result in large changes in the estimated coefficients, making them unstable and unreliable.\n",
    "\n",
    "Reduced Interpretability: High multicollinearity can make it difficult to interpret the impact of individual predictors on the dependent variable because the relationships between variables become intertwined.\n",
    "\n",
    "Inconsistent Sign and Magnitude: In some cases, the signs of coefficients can be inconsistent with theory or intuition, and the magnitude of the coefficients can become distorted.\n",
    "\n",
    "Reduced Predictive Accuracy: Multicollinearity can lead to less accurate predictions and make it challenging for the model to distinguish the unique contributions of each predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ff8af",
   "metadata": {},
   "source": [
    "b. Use correlation matrices or variance inflation factors (VIFs) to identify multicollinearity in your predictor variables:\n",
    "\n",
    "You can use correlation matrices or Variance Inflation Factors (VIFs) to detect multicollinearity:\n",
    "\n",
    "Correlation Matrices:\n",
    "\n",
    "Calculate the correlation coefficients (e.g., Pearson's correlation) between pairs of independent variables. Correlation coefficients close to 1 or -1 indicate high multicollinearity between those variables.\n",
    "Variance Inflation Factors (VIFs):\n",
    "\n",
    "Calculate the VIF for each independent variable. VIF measures how much the variance of the estimated regression coefficient is increased due to multicollinearity. A VIF greater than 1 indicates the presence of multicollinearity.\n",
    "The formula to calculate VIF for each variable X_i is: VIF(X_i) = 1 / (1 - R^2), where R^2 is the coefficient of determination from a regression of X_i against all other independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313cd955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Calculate VIF for each independent variable\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Variable\"] = X_train.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
    "\n",
    "# Identify variables with high VIF (typically VIF > 5 is a sign of significant multicollinearity)\n",
    "high_vif_vars = vif[vif[\"VIF\"] > 5]\n",
    "\n",
    "# Print the variables with high VIF\n",
    "print(\"Variables with high VIF:\")\n",
    "print(high_vif_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60770f04",
   "metadata": {},
   "source": [
    "# 5. Mitigating Multicollinearity:\n",
    "\n",
    "a Discuss the potential issues associated with multicollinearity and its impact on model interpretability.\n",
    "\n",
    "b. Propose strategies for mitigating multicolinearity, such as feature selection or regularization techniques\n",
    "\n",
    "c Implement the chosen strategy to reduce multicollineanty and analyze the model's performance after the adjustments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b94647",
   "metadata": {},
   "source": [
    "a. Discuss the potential issues associated with multicollinearity and its impact on model interpretability:\n",
    "\n",
    "Multicollinearity can have several adverse effects on a linear regression model and its interpretability:\n",
    "\n",
    "Inflated Standard Errors: Multicollinearity leads to inflated standard errors for the coefficients, making it challenging to assess the statistical significance of individual predictors. This can result in wider confidence intervals and p-values that do not accurately reflect the significance of predictors.\n",
    "\n",
    "Unstable Coefficients: Small changes in the data can cause large variations in the estimated coefficients, making them unstable and unreliable for interpretation.\n",
    "\n",
    "Interpretation Challenges: High multicollinearity makes it difficult to interpret the impact of individual predictors on the dependent variable, as the relationships between predictors become intertwined.\n",
    "\n",
    "Inconsistent Sign and Magnitude: In some cases, the signs of coefficients may be inconsistent with theory or intuition, and the magnitude of coefficients can become distorted.\n",
    "\n",
    "Reduced Predictive Accuracy: Multicollinearity can lead to less accurate predictions, making it challenging for the model to distinguish the unique contributions of each predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f3126c",
   "metadata": {},
   "source": [
    "b. Propose strategies for mitigating multicollinearity:\n",
    "\n",
    "There are several strategies to mitigate multicollinearity:\n",
    "\n",
    "Feature Selection: Remove one or more of the highly correlated variables from the model. This is often done based on domain knowledge or feature importance scores. The goal is to retain the most relevant variables while removing redundant ones.\n",
    "\n",
    "Regularization Techniques: Regularized regression techniques like Lasso (L1 regularization) and Ridge (L2 regularization) can automatically handle multicollinearity by shrinking the coefficients. Lasso, in particular, can drive some coefficients to zero, effectively selecting a subset of predictors.\n",
    "\n",
    "Principal Component Analysis (PCA): PCA is a dimensionality reduction technique that can be used to transform the original predictors into a new set of uncorrelated variables (principal components). This can help reduce multicollinearity, but it comes at the cost of interpretability.\n",
    "\n",
    "Partial Least Squares (PLS): PLS is a supervised dimensionality reduction technique that aims to maximize the covariance between predictors and the target variable. It can be used to reduce multicollinearity while maintaining some level of interpretability.\n",
    "\n",
    "Combine Variables: If two or more highly correlated variables are conceptually similar, consider creating a composite variable by averaging or summing them. This can be used as a single predictor in the model.\n",
    "\n",
    "Interaction Terms: In some cases, creating interaction terms between highly correlated variables can help capture their joint effects while reducing multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc2ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Create and fit a Lasso regression model\n",
    "lasso_model = Lasso(alpha=0.01)  # Adjust alpha as needed\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Lasso model's performance\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "print(\"Mean Squared Error (Lasso):\", mse_lasso)\n",
    "print(\"R-squared (Lasso):\", r2_lasso)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041dbad2",
   "metadata": {},
   "source": [
    "# 6. Model Evaluation:\n",
    "\n",
    "a. Evaluate the overall performance of your improved model in terms of metrics like R-squared MAE, MSE, and RMSE.\n",
    "\n",
    "b. Discuss the significance of the model's coefficients and their interpretations after addressing heteroscedasticity and multicollinearity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb93f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have your improved model and predictions (e.g., lasso_model and y_pred_lasso)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test, y_pred_lasso)\n",
    "mse = mean_squared_error(y_test, y_pred_lasso)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-squared (R2):\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5774b0",
   "metadata": {},
   "source": [
    "b. Discuss the significance of the model's coefficients and their interpretations after addressing heteroscedasticity and multicollinearity:\n",
    "\n",
    "After addressing heteroscedasticity and multicollinearity, the model's coefficients become more reliable and interpretable. Here's how to interpret them:\n",
    "\n",
    "Lasso Coefficients: In the Lasso regression model, some coefficients may have been driven to zero due to L1 regularization. The non-zero coefficients represent the variables that have the most significant impact on predicting employee performance. The signs and magnitudes of these coefficients indicate the direction and strength of the relationships between predictors and the target variable.\n",
    "\n",
    "Interpreting Coefficients: The coefficient for each variable represents how a one-unit change in that variable impacts the dependent variable while holding all other variables constant. For example, if the coefficient for \"Experience\" is 0.5, it means that a one-year increase in experience is associated with a 0.5 unit increase in employee performance, all else being equal.\n",
    "\n",
    "Reduced Collinearity Effects: Addressing multicollinearity makes the coefficients more stable and interpretable. You can more confidently state the unique contribution of each predictor to the model's predictions.\n",
    "\n",
    "Impact of Transformations: If you applied transformations to variables to mitigate heteroscedasticity, be sure to interpret the coefficients in the context of the transformed variables. For example, if you took the square root of a variable, the coefficient's interpretation should account for that transformation.\n",
    "\n",
    "Standard Errors: With reduced multicollinearity, the standard errors of the coefficients are more reliable. You can use these standard errors to assess the statistical significance of each predictor.\n",
    "\n",
    "Adjusted R-squared: Compare the adjusted R-squared value of the model before and after addressing multicollinearity. A higher adjusted R-squared suggests that the model explains more of the variation in employee performance while using fewer variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba9b964",
   "metadata": {},
   "source": [
    "# 7. Conclusion:\n",
    "\n",
    "a. Summarize the impact of identifying and addressing heteroscedasticity and multicollinearity on the predictive accuracy and interpretability of your employee performance model.\n",
    "\n",
    "b. Provide recommendations for future model development and potential areas for further Improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76269279",
   "metadata": {},
   "source": [
    "a. Summarize the impact of identifying and addressing heteroscedasticity and multicollinearity on the predictive accuracy and interpretability of your employee performance model:\n",
    "\n",
    "Identifying and addressing heteroscedasticity and multicollinearity had a significant positive impact on the predictive accuracy and interpretability of the employee performance model. Here are the key takeaways:\n",
    "\n",
    "Improved Predictive Accuracy: By addressing heteroscedasticity, the model's predictive accuracy increased, as it no longer gave undue weight to observations with high variability. This means the model is better at making accurate predictions of employee performance.\n",
    "\n",
    "Enhanced Model Interpretability: Mitigating multicollinearity made the model more interpretable. The coefficients of the predictors became more stable and easier to interpret, allowing us to clearly understand the influence of each variable on employee performance.\n",
    "\n",
    "Statistical Significance: With reduced multicollinearity, the statistical significance of the coefficients became more reliable. We can have greater confidence in the direction and magnitude of the relationships between predictors and employee performance.\n",
    "\n",
    "More Efficient Model: The adjusted model likely uses fewer variables while explaining a similar or greater proportion of the variation in employee performance. This results in a more efficient and parsimonious model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0962f58c",
   "metadata": {},
   "source": [
    "b. Provide recommendations for future model development and potential areas for further improvement:\n",
    "\n",
    "Explore Nonlinear Relationships: Consider investigating potential nonlinear relationships between predictors and employee performance. You can use polynomial regression or other nonlinear models to capture more complex interactions.\n",
    "\n",
    "Incorporate Additional Features: Explore the inclusion of additional features that may have a significant impact on employee performance. These could include variables related to job satisfaction, work environment, or other relevant factors.\n",
    "\n",
    "Temporal Analysis: If available, consider analyzing employee performance data over time to identify trends or patterns that can enhance predictive accuracy. Time series or longitudinal analysis might be valuable.\n",
    "\n",
    "Feature Engineering: Continue to refine feature engineering techniques to create more informative predictor variables. This can involve creating interaction terms, deriving new features, or transforming existing ones.\n",
    "\n",
    "Regularization Tuning: Experiment with different regularization techniques (e.g., different alpha values in Lasso) to find the optimal balance between model complexity and predictive accuracy.\n",
    "\n",
    "Cross-Validation: Implement cross-validation to assess the model's generalization performance more effectively and ensure it performs well on new, unseen data.\n",
    "\n",
    "Collect More Data: If feasible, collect more data to increase the model's training sample size, which can lead to more robust and accurate predictions.\n",
    "\n",
    "Model Comparison: Compare the improved linear regression model with other machine learning models (e.g., decision trees, random forests, gradient boosting) to determine which one provides the best predictive accuracy.\n",
    "\n",
    "Domain Expertise: Involve domain experts in the model development process to gain valuable insights into the factors affecting employee performance and to guide feature selection and engineering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
