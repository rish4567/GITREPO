{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05dcb469",
   "metadata": {},
   "source": [
    "#  Decision Tree for Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04231283",
   "metadata": {},
   "source": [
    "# You work for a data-driven company that focuses on predicting customer satisfaction and sales for a retail business. Your task is to build predictive models using decision trees. The dataset contains various customer attributes, shopping behavior, and survey responses. Answer the following questions based on this case study: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2eb9a0",
   "metadata": {},
   "source": [
    "# 1. Data Exploration: \n",
    "     a. Load the dataset using Python libraries like pandas and explore its structure. Describe the features, target variables and data distribution. \n",
    "    \n",
    "     b. Discuss the importance of customer satisfaction and sales prediction in the retail business context. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "data = pd.read_csv('Airline_Customer.csv')\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "print()\n",
    "# Describe the features, target variables, and data distribution\n",
    "print(\"Dataset Summary:\")\n",
    "print(data.info())\n",
    "\n",
    "print()\n",
    "# Summary statistics of numerical features\n",
    "print(\"Summary Statistics of Numerical Features:\")\n",
    "print(data.describe())\n",
    "\n",
    "print()\n",
    "# Summary statistics of categorical features\n",
    "print(\"Summary Statistics of Categorical Features:\")\n",
    "print(data.describe(include=['object']))\n",
    "\n",
    "print()\n",
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265208a4",
   "metadata": {},
   "source": [
    "\n",
    "Customer Satisfaction:\n",
    "    Understanding customer satisfaction levels (e.g., \"satisfied\") is crucial. \n",
    "    Satisfied customers enhance brand loyalty and positive word-of-mouth, shaping a positive brand image. \n",
    "    Their feedback informs improvements, optimizing services and products.\n",
    "\n",
    "Sales Prediction:\n",
    "    Accurate sales prediction, derived from features like flight distance and service ratings, aids inventory management and demand forecasting. \n",
    "    It optimizes marketing efforts, ensuring resources are allocated effectively. \n",
    "    Predictions facilitate financial planning and strategic adjustments during seasonal fluctuations, maximizing revenue and ensuring business sustainability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec5c6b6",
   "metadata": {},
   "source": [
    "# 2. Classification Task - Predicting Customer Satisfaction: \n",
    "    a. Implement a decision tree classifier using Python libraries like scikit-learn to predict customer satisfaction. \n",
    "    b. Split the dataset into training and testing sets and train the model. \n",
    "    c. Evaluate the classification model's performance using relevant metrics such as accuracy, precision, recall, and F1-score. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db03f23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Airline_Customer.csv')\n",
    "\n",
    "# Handle missing values in 'Arrival Delay in Minutes' column by replacing with mean\n",
    "data['Arrival Delay in Minutes'].fillna(data['Arrival Delay in Minutes'].mean(), inplace=True)\n",
    "\n",
    "# Features (X) and target variable (y)\n",
    "X = data.drop(columns=['satisfaction'])\n",
    "y = data['satisfaction']\n",
    "\n",
    "#b. Split the dataset into training and testing sets\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize and train the decision tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(xtrain, ytrain)\n",
    "\n",
    "# Predictions on the test set\n",
    "ypred = dt_classifier.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c9e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b.  Evaluate the classification model's performance\n",
    "accuracy = accuracy_score(ytest, ypred)\n",
    "precision = precision_score(ytest, ypred, pos_label='satisfied')\n",
    "recall = recall_score(ytest, ypred, pos_label='satisfied')\n",
    "f1 = f1_score(ytest, ypred, pos_label='satisfied')\n",
    "\n",
    "#  Print the evaluation metrics\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-Score: {:.2f}\".format(f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ab4975",
   "metadata": {},
   "source": [
    "# 3. Regression Task - Predicting Sales: \n",
    "    a. Implement a decision tree regression model using Python libraries to predict sales based on customer attributes and behavior.  \n",
    "    \n",
    "    b. Discuss the differences between classification and regression tasks in predictive modeling. \n",
    "\n",
    "    c. Split the dataset into training and testing sets and train the regression model. \n",
    "   \n",
    "    d. Evaluate the regression model's performance using metrics such as mean squared error (MSE) and R-squared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174fdcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.read_csv('Airline_Customer.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a360db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the sample data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize label encoder for target variable\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the target variable 'satisfaction'\n",
    "df['satisfaction'] = label_encoder.fit_transform(df['satisfaction'])\n",
    "\n",
    "# Select features and target variable\n",
    "features = ['Age', 'Flight Distance']  # Add more features as needed\n",
    "X = df[features]\n",
    "y = df['satisfaction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0102541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c. Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the decision tree regressor\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ef3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. evaluating the mean squared error and R-squared score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9eab11",
   "metadata": {},
   "source": [
    "# 4. Decision Tree Visualization: \n",
    " \n",
    "a. Visualize the decision tree for both the classification and regression models. Discuss the interpretability of decision trees in predictive modeling. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71543315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('Airline_Customer.csv')\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = ['Age', 'Flight Distance']\n",
    "categorical_features = ['satisfaction']  # Assuming 'satisfaction' is the categorical column\n",
    "\n",
    "# Handling missing values for numeric features by imputing with mean\n",
    "numeric_imputer = SimpleImputer(strategy='mean')\n",
    "data[numeric_features] = numeric_imputer.fit_transform(data[numeric_features])\n",
    "\n",
    "# Handling missing values for categorical features by imputing with the most frequent value\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "data[categorical_features] = categorical_imputer.fit_transform(data[categorical_features])\n",
    "\n",
    "# Initialize label encoder for target variable\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the target variable 'satisfaction'\n",
    "data['satisfaction'] = label_encoder.fit_transform(data['satisfaction'])\n",
    "\n",
    "# Select features and target variable\n",
    "features = numeric_features  # Add more features as needed\n",
    "X = data[features]\n",
    "y = data['satisfaction']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the decision tree classifier\n",
    "dt_clf = DecisionTreeClassifier(criterion='entropy', max_depth=2, random_state=42)\n",
    "\n",
    "# Train the classification model\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_class = dt_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classification model\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "print(\"Classification Model Accuracy:\", accuracy)\n",
    "\n",
    "# Visualize the decision tree for the classification model\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_tree(dt_clf, filled=True, feature_names=numeric_features, class_names=label_encoder.classes_)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Flight Distance\")\n",
    "plt.title(\"Decision Tree Classification Model\")\n",
    "plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('Airline_Customer.csv')\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = ['Age', 'Flight Distance']\n",
    "categorical_features = ['satisfaction']  # Assuming 'satisfaction' is the categorical column\n",
    "\n",
    "# Handling missing values for numeric features by imputing with mean\n",
    "numeric_imputer = SimpleImputer(strategy='mean')\n",
    "data[numeric_features] = numeric_imputer.fit_transform(data[numeric_features])\n",
    "\n",
    "# Handling missing values for categorical features by imputing with the most frequent value\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "data[categorical_features] = categorical_imputer.fit_transform(data[categorical_features])\n",
    "\n",
    "# Initialize label encoder for target variable\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the target variable 'satisfaction'\n",
    "data['satisfaction'] = label_encoder.fit_transform(data['satisfaction'])\n",
    "\n",
    "# Select features and target variable\n",
    "features = numeric_features  # Add more features as needed\n",
    "X = data[features]\n",
    "y = data['satisfaction']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the decision tree regressor\n",
    "dt_regressor = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "\n",
    "# Train the regression model\n",
    "dt_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_reg = dt_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the regression model\n",
    "mse = mean_squared_error(y_test, y_pred_reg)\n",
    "r2 = r2_score(y_test, y_pred_reg)\n",
    "print(\"Regression Model Mean Squared Error:\", mse)\n",
    "print(\"Regression Model R-squared:\", r2)\n",
    "\n",
    "# Visualize the decision tree for the regression model\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_tree(dt_regressor, filled=True, feature_names=numeric_features, class_names=label_encoder.classes_)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Flight Distance\")\n",
    "plt.title(\"Decision Tree Regression Model\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a2cab",
   "metadata": {},
   "source": [
    "# 5. Feature Importance:\n",
    "  a. Determine the most important features in both models by examining the decision tree structure. Discuss how feature importance is calculated in decision trees. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dddff31",
   "metadata": {},
   "source": [
    "\n",
    "In decision trees, feature importance is calculated based on how much each feature contributes to reducing impurity or entropy in the nodes of the tree. The importance of a feature is determined by calculating the weighted average of impurity decrease across all nodes where the feature is used for splitting. Features that result in nodes with lower impurity are considered more important.\n",
    "\n",
    "To determine the most important features in both models:\n",
    "\n",
    "For Classification Model:\n",
    "\n",
    "Examine the nodes in the decision tree where the splits occur.\n",
    "Features used in higher nodes (closer to the root) and result in pure or nearly pure child nodes are more important.\n",
    "Calculate impurity decrease or information gain for each split and aggregate the importance scores for each feature.\n",
    "\n",
    "For Regression Model:\n",
    "\n",
    "Similar to the classification model, focus on nodes where splits occur.\n",
    "Calculate the reduction in mean squared error (MSE) or variance for each split.\n",
    "Features leading to nodes with significant reduction in MSE are considered more important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020ed91e",
   "metadata": {},
   "source": [
    "# 6. Overfitting and Pruning: \n",
    "    a. Explain the concept of overfitting in the context of decision trees. \n",
    "    b. Discuss methods for reducing overfitting, such as pruning, minimum samples per leaf, and maximum depth. \n",
    "    c. Implement pruning or other techniques as necessary and analyze their impact on the model's performance. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd61afa",
   "metadata": {},
   "source": [
    " a. Explain the concept of overfitting in the context of decision trees. \n",
    "\n",
    "    Overfitting in Decision Trees:\n",
    "    \n",
    "    Overfitting occurs when a decision tree model captures noise and specific patterns in the training data to an extent that it negatively impacts its performance on unseen data. \n",
    "    In the context of decision trees, an overfit tree is excessively complex, capturing noise as if it were a real pattern. \n",
    "    Such a tree may have many branches and nodes that are tailored to the training data but do not generalize well to new, unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c33a20",
   "metadata": {},
   "source": [
    " b. Discuss methods for reducing overfitting, such as pruning, minimum samples per leaf, and maximum depth. \n",
    "\n",
    "Methods to Reduce Overfitting:\n",
    "\n",
    "    Pruning: Pruning involves removing parts of the tree that do not provide significant predictive power. \n",
    "             It can be done by setting constraints on the tree's structure, such as maximum depth or minimum samples per leaf.\n",
    "    \n",
    "    Minimum Samples per Leaf: Setting a minimum number of samples required to be at a leaf node can prevent the creation of nodes that are too specific to the training data.\n",
    "    \n",
    "    Maximum Depth: Limiting the depth of the tree prevents it from becoming too intricate, as deeper trees are more likely to overfit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba83ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Implement pruning or other techniques as necessary and analyze their impact on the model's performance. \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('Airline_Customer.csv')\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = ['Age', 'Flight Distance']\n",
    "categorical_features = ['satisfaction']  # Assuming 'satisfaction' is the categorical column\n",
    "\n",
    "# Initialize label encoder for target variable\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the target variable 'satisfaction'\n",
    "data['satisfaction'] = label_encoder.fit_transform(data['satisfaction'])\n",
    "\n",
    "# Select features and target variable\n",
    "features = numeric_features  # Add more features as needed\n",
    "X = data[features]\n",
    "y = data['satisfaction']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the decision tree classifier with pruning (max_depth=5)\n",
    "dt_classifier_pruned = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=42)\n",
    "\n",
    "# Train the pruned classification model\n",
    "dt_classifier_pruned.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_class_pruned = dt_classifier_pruned.predict(X_test)\n",
    "\n",
    "# Evaluate the pruned classification model\n",
    "accuracy_pruned = accuracy_score(y_test, y_pred_class_pruned)\n",
    "print(\"Pruned Classification Model Accuracy:\", accuracy_pruned)\n",
    "\n",
    "# Initialize the decision tree regressor with pruning (max_depth=5)\n",
    "dt_regressor_pruned = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "\n",
    "# Train the pruned regression model\n",
    "dt_regressor_pruned.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_reg_pruned = dt_regressor_pruned.predict(X_test)\n",
    "\n",
    "# Evaluate the pruned regression model\n",
    "mse_pruned = mean_squared_error(y_test, y_pred_reg_pruned)\n",
    "r2_pruned = r2_score(y_test, y_pred_reg_pruned)\n",
    "print(\"Pruned Regression Model Mean Squared Error:\", mse_pruned)\n",
    "print(\"Pruned Regression Model R-squared:\", r2_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdbf0b9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After implementing pruning techniques, the impact on the model's performance can be analyzed as follows:\n",
    "\n",
    "Classification Model:\n",
    "    \n",
    "    Original Accuracy: 58.29%\n",
    "    Pruned Accuracy: 62.51%\n",
    "        \n",
    "    The classification model's accuracy improved from approximately 58.29% to 62.51% after pruning. \n",
    "    Pruning enhanced the model's ability to correctly classify customer satisfaction levels, indicating a positive impact on the classification model's performance.\n",
    "\n",
    "Regression Model:\n",
    "    Original Mean Squared Error: 0.2278\n",
    "    Original R-squared: 0.0794\n",
    "    Pruned Mean Squared Error: 0.2278\n",
    "    Pruned R-squared: 0.0794\n",
    "    \n",
    "    For the regression model, pruning did not impact the mean squared error or R-squared values significantly. \n",
    "    The mean squared error remained the same at approximately 0.2278, and the R-squared value remained around 0.0794. \n",
    "    This suggests that pruning didn't have a notable impact on the regression model's performance in terms of prediction accuracy and explained variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0338503b",
   "metadata": {},
   "source": [
    "# 7. Real-World Application: \n",
    "   a. Describe the practical applications of customer satisfaction prediction and sales forecasting in the retail industry. \n",
    "   \n",
    "   b. Discuss the potential benefits of using predictive models in retail business operations and decision-making. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d4b7b3",
   "metadata": {},
   "source": [
    "\n",
    "a. Customer Satisfaction Prediction in Retail:\n",
    "\n",
    "    Personalized Marketing: Predicting customer satisfaction helps in tailoring marketing strategies. Satisfied customers can be targeted with loyalty programs, while dissatisfied customers can receive special offers to improve their experience.\n",
    "    Product Improvement: Analyzing customer feedback aids in product enhancements. Identifying patterns in dissatisfaction helps retailers make necessary adjustments to products or services.\n",
    "    Customer Retention: Predictive models can foresee customer churn. Retailers can proactively address concerns, improving customer retention and long-term revenue.\n",
    "b. Sales Forecasting in Retail:\n",
    "\n",
    "    Inventory Management: Accurate sales forecasts optimize inventory levels. Retailers can avoid overstocking or stockouts, ensuring products are available when customers demand them.\n",
    "    Staffing and Operations: Predicting busy periods allows retailers to schedule staff efficiently, ensuring there are enough employees during peak hours, enhancing customer service.\n",
    "    Supply Chain Optimization: Suppliers and logistics can be informed about anticipated demand, streamlining the supply chain, reducing costs, and minimizing wastage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d827982",
   "metadata": {},
   "source": [
    "b. Benefits of Predictive Models in Retail:\n",
    "\n",
    "    Enhanced Customer Experience: Predictive models allow retailers to understand customer preferences, enabling personalized shopping experiences, increasing customer satisfaction and loyalty.\n",
    "    Optimized Inventory Management: Accurate forecasts prevent excess inventory or shortages, reducing carrying costs and maximizing profits.\n",
    "    Improved Decision-making: Data-driven insights facilitate informed decisions in pricing, promotions, and marketing, ensuring resources are allocated efficiently.\n",
    "    Competitive Advantage: Retailers using predictive analytics stay ahead of market trends, outperform competitors, and adapt swiftly to changing customer demands.\n",
    "    Cost Efficiency: By minimizing inefficiencies in operations, retailers can optimize costs and invest resources strategically, ensuring higher profitability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c778617",
   "metadata": {},
   "source": [
    "# 8. Model Comparison: \n",
    "    a. Compare the performance of the decision tree classification and regression models. \n",
    "    b. Discuss the trade-offs, advantages, and limitations of decision trees for different types of predictive tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cae2849",
   "metadata": {},
   "source": [
    "a. Compare the performance of the decision tree classification and regression models. \n",
    "\n",
    "Performance Comparison:\n",
    "\n",
    "    Original Decision Tree Models:\n",
    "\n",
    "        Classification Model Accuracy: 58.3%\n",
    "        Regression Model Mean Squared Error: 0.2355\n",
    "        Regression Model R-squared: 4.8%\n",
    "\n",
    "    Pruned Decision Tree Models:\n",
    "\n",
    "        Pruned Classification Model Accuracy: 62.5%\n",
    "        Pruned Regression Model Mean Squared Error: 0.2278\n",
    "        Pruned Regression Model R-squared: 7.9%\n",
    "\n",
    "Comparison Insights:\n",
    "\n",
    "    Classification Model:\n",
    "\n",
    "        The pruned decision tree classification model outperforms the original model, showing an improvement in accuracy from 58.3% to 62.5%. Pruning helped enhance the model's performance, reducing overfitting.\n",
    "\n",
    "    Regression Model:\n",
    "\n",
    "        The pruned decision tree regression model exhibits a lower mean squared error (0.2278) and a slightly higher R-squared value (7.9%) compared to the original regression model (mean squared error: 0.2355, R-squared: 4.8%). Pruning resulted in a more accurate and better-fitting regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a89da",
   "metadata": {},
   "source": [
    "b. Discuss the trade-offs, advantages, and limitations of decision trees for different types of predictive tasks. \n",
    "\n",
    "\n",
    "Trade-offs in Our Models:\n",
    "\n",
    "    Interpretability vs. Complexity: The decision tree models you built are relatively shallow, balancing interpretability and complexity. Shallow trees are less likely to overfit.\n",
    "    Bias vs. Variance: The models' performance suggests a balance between bias and variance, indicating that they generalize reasonably well to unseen data.\n",
    "\n",
    "Advantages in Our Models:\n",
    "\n",
    "    Interpretability: Both classification and regression decision trees are interpretable, allowing easy understanding of the factors influencing customer satisfaction and sales prediction.\n",
    "    Handling Non-linearity: Decision trees excel at capturing non-linear relationships, making them suitable for modeling complex customer behavior and sales patterns.\n",
    "    Mixed Data Types: Your models successfully handled a mix of numerical (age, flight distance) and categorical (satisfaction) features without extensive preprocessing.\n",
    "\n",
    "Limitations in Our Models:\n",
    "\n",
    "    Overfitting: While you implemented pruning to address overfitting, it's essential to carefully tune hyperparameters to avoid creating overly complex trees that capture noise.\n",
    "    Sensitivity to Small Data Variations: Decision trees can be sensitive to small variations in the training data, potentially leading to different tree structures for similar datasets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
