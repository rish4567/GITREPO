{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb488738",
   "metadata": {},
   "source": [
    "# Implement a multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4848805",
   "metadata": {},
   "source": [
    "# \" Linearlayer perceptron\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca4537a",
   "metadata": {},
   "source": [
    "A perceptron is a basic neural network unit that can be used for binary classification tasks. Here's a Python program for a linear perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67476571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0], Predicted Output: 0\n",
      "Input: [0 1], Predicted Output: 0\n",
      "Input: [1 0], Predicted Output: 0\n",
      "Input: [1 1], Predicted Output: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1):\n",
    "        self.weights = np.random.rand(input_size)\n",
    "        self.bias = np.random.rand()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
    "        return 1 if weighted_sum >= 0 else 0\n",
    "\n",
    "    def train(self, training_data, labels, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(training_data)):\n",
    "                prediction = self.predict(training_data[i])\n",
    "                error = labels[i] - prediction\n",
    "                self.weights += self.learning_rate * error * training_data[i]\n",
    "                self.bias += self.learning_rate * error\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define training data and labels (AND gate example)\n",
    "    training_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    labels = np.array([0, 0, 0, 1])\n",
    "\n",
    "    # Create a perceptron with 2 input neurons (for the AND gate example)\n",
    "    perceptron = Perceptron(input_size=2)\n",
    "\n",
    "    # Train the perceptron for 100 epochs\n",
    "    perceptron.train(training_data, labels, epochs=100)\n",
    "\n",
    "    # Test the trained perceptron\n",
    "    test_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    for data in test_data:\n",
    "        prediction = perceptron.predict(data)\n",
    "        print(f\"Input: {data}, Predicted Output: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b26971",
   "metadata": {},
   "source": [
    "This program creates a simple perceptron with adjustable learning rate and trains it on the AND gate dataset. After training, it tests the perceptron on the same dataset and prints the predicted outputs. You can modify the training_data and labels to use different datasets for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ea3d3",
   "metadata": {},
   "source": [
    "# \"Implement a multilayer perceptron\" with the use of linearlayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d665d78d",
   "metadata": {},
   "source": [
    "To implement a multilayer perceptron (MLP), you can extend the previous example to include additional layers with multiple neurons. An MLP consists of an input layer, one or more hidden layers, and an output layer. Each neuron in a layer is connected to all neurons in the previous and subsequent layers. Here's a simple Python program that demonstrates the implementation of a multilayer perceptron for a binary classification task using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a87411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights_hidden = np.random.rand(input_size, hidden_size)\n",
    "        self.bias_hidden = np.random.rand(hidden_size)\n",
    "        self.weights_output = np.random.rand(hidden_size, output_size)\n",
    "        self.bias_output = np.random.rand(output_size)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.hidden_layer_input = np.dot(inputs, self.weights_hidden) + self.bias_hidden\n",
    "        self.hidden_layer_output = self.sigmoid(self.hidden_layer_input)\n",
    "        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_output) + self.bias_output\n",
    "        self.output_layer_output = self.sigmoid(self.output_layer_input)\n",
    "        return self.output_layer_output\n",
    "\n",
    "    def backward(self, inputs, labels, output):\n",
    "        error_output = labels - output\n",
    "        delta_output = error_output * self.sigmoid_derivative(output)\n",
    "\n",
    "        error_hidden = delta_output.dot(self.weights_output.T)\n",
    "        delta_hidden = error_hidden * self.sigmoid_derivative(self.hidden_layer_output)\n",
    "\n",
    "        self.weights_output += self.hidden_layer_output.T.dot(delta_output) * self.learning_rate\n",
    "        self.bias_output += np.sum(delta_output, axis=0, keepdims=True) * self.learning_rate\n",
    "        self.weights_hidden += inputs.T.dot(delta_hidden) * self.learning_rate\n",
    "        self.bias_hidden += np.sum(delta_hidden, axis=0, keepdims=True) * self.learning_rate\n",
    "\n",
    "    def train(self, training_data, labels, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(training_data)):\n",
    "                inputs = training_data[i]\n",
    "                output = self.forward(inputs)\n",
    "                self.backward(inputs, labels[i], output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    labels = np.array([[0], [1], [1], [0]])\n",
    "    mlp = MLP(input_size=2, hidden_size=2, output_size=1)\n",
    "    mlp.train(training_data, labels, epochs=10000)\n",
    "    test_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    for data in test_data:\n",
    "        prediction = mlp.forward(data)\n",
    "        print(f\"Input: {data}, Predicted Output: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd6900",
   "metadata": {},
   "source": [
    "In this program, we've extended the MLP class to include a forward and backward pass for training. The program demonstrates training an MLP on the XOR gate dataset, and you can adjust the architecture, dataset, and training parameters to suit your specific classification task. The example uses a single hidden layer with two neurons, but you can add more hidden layers and neurons to create deeper networks for more complex tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
